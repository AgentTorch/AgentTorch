{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archetype Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Setup\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_torch.core.llm.archetype import Archetype\n",
    "from agent_torch.core.llm.behavior import Behavior\n",
    "from agent_torch.populations import NYC\n",
    "from agent_torch.core.llm.backend import LangchainLLM\n",
    "\n",
    "OPENAI_API_KEY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup : Covid Cases Data and Unemployment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_covid_cases_data\n",
    "\n",
    "csv_path = \"/models/covid/data/county_data.csv\"\n",
    "monthly_cases_kings = get_covid_cases_data(\n",
    "    csv_path=csv_path, county_name=\"Kings County\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Initialise LLM Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use either of the Langchain and Dspy backends to initialise a LLM instance. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use Langchain to initialise an LLM instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n",
    "llm_langchain_35 = LangchainLLM(\n",
    "    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define an Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API: Define a Template and use Archetype directly (no Behavior object)\n",
    "import agent_torch.core.llm.template as lm\n",
    "from agent_torch.core.llm.mock_llm import MockLLM\n",
    "import agent_torch.populations.astoria as astoria\n",
    "import pandas as pd\n",
    "\n",
    "class DemoTemplate(lm.Template):\n",
    "    system_prompt = \"You are evaluating willingness based on job profile and context.\"\n",
    "    age = lm.Variable(desc=\"agent age\", learnable=True)\n",
    "    gender = lm.Variable(desc=\"agent gender\", learnable=False)\n",
    "    soc_code = lm.Variable(desc=\"job id\", learnable=False)\n",
    "    abilities = lm.Variable(desc=\"abilities required\", learnable=True)\n",
    "    work_context = lm.Variable(desc=\"work context\", learnable=True)\n",
    "\n",
    "    def __prompt__(self):\n",
    "        self.prompt_string = (\n",
    "            \"You are in your {age}'s and are a {gender}. \"\n",
    "            \"As a {soc_code}...you have {abilities} and work in {work_context}.\"\n",
    "        )\n",
    "\n",
    "    def __output__(self):\n",
    "        return \"Rate your willingness to continue normal activities, respond in [0, 1] binary decision only.\"\n",
    "\n",
    "# Create Archetype\n",
    "llm = MockLLM()\n",
    "arch = Archetype(prompt=DemoTemplate(), llm=llm, n_arch=3)\n",
    "\n",
    "# Configure external data and ground truth\n",
    "all_jobs_df = pd.read_pickle(\"job_data_clean.pkl\")\n",
    "gt_csv = pd.read_csv(\"agent_torch/core/llm/data/ground_truth_willingness_all_soc.csv\")\n",
    "soc_to_val = {str(r['soc_code']): float(r['willingness']) for _, r in gt_csv.iterrows()}\n",
    "gt_list = [soc_to_val.get(str(row.get('soc_code')), 0.0) for _, row in all_jobs_df.iterrows()]\n",
    "\n",
    "arch.configure(\n",
    "    external_df=all_jobs_df,\n",
    "    ground_truth=gt_list,\n",
    "    match_on=\"soc_code\",\n",
    ")\n",
    "\n",
    "# Sample before and after broadcast\n",
    "arch.sample(print_examples=3)\n",
    "arch.broadcast(population=astoria)\n",
    "arch.sample(print_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P3O Optimization (New API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Template variables using P3O\n",
    "from agent_torch.optim import P3O\n",
    "\n",
    "# Create optimizer; default uses PSPGO-shaped reward\n",
    "opt = P3O(arch.parameters(), archetype=arch)\n",
    "\n",
    "# Run a few optimization steps over the broadcast population\n",
    "for _ in range(3):\n",
    "    arch.sample()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "# Inspect learnable parameter count\n",
    "len(list(arch.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments to be used for creating a query for the LLM Instance\n",
    "kwargs = {\n",
    "    \"month\": \"January\",\n",
    "    \"year\": \"2020\",\n",
    "    \"covid_cases\": 1200,\n",
    "    \"device\": \"cpu\",\n",
    "    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n",
    "    \"unemployment_rate\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compare performance between different Configurations of Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labor_data, get_labor_force_correlation\n",
    "\n",
    "labor_force_df_n_2, observed_labor_force_n_2, correlation_n_2 = (\n",
    "    get_labor_force_correlation(\n",
    "        monthly_cases_kings,\n",
    "        earning_behavior_n_2,\n",
    "        \"agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv\",\n",
    "        kwargs,\n",
    "    )\n",
    ")\n",
    "labor_force_df_n_12, observed_labor_force_n_12, correlation_n_12 = (\n",
    "    get_labor_force_correlation(\n",
    "        monthly_cases_kings,\n",
    "        earning_behavior_n_12,\n",
    "        \"agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv\",\n",
    "        kwargs,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    f\"Correlation with 2 Archetypes is {correlation_n_2} and 12 Archetypes is {correlation_n_12}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
