{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Archetypes (New API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Related reading\n",
    "> - ../index.md\n",
    "> - P3O tutorial: ../optimizing-on-prompts/index.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Setup\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_torch.core.llm.archetype import Archetype\n",
    "from agent_torch.core.llm.mock_llm import MockLLM\n",
    "import agent_torch.populations.astoria as astoria\n",
    "import pandas as pd, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Initialise LLM Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use any LLM, as long as it implements the required __call__(prompt_inputs) hook.\n",
    "\n",
    "- Required: __call__(prompt_inputs: list[{\"agent_query\": str, \"chat_history\": list}]) -> list[str|{\"text\": str}]\n",
    "- Optional: initialize_llm(self) for one-time setup\n",
    "- Output must be float convertible, view index.md for more details.\n",
    "\n",
    "AgentTorch supplies per-agent chat_history and parses outputs into floats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example minimal implementation for GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "class ChatGPT35LLM:\n",
    "    def __init__(self, api_key=None, model=\"gpt-3.5-turbo\", temperature=0.2):\n",
    "        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = None\n",
    "\n",
    "    def initialize_llm(self):\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "        return self\n",
    "\n",
    "    def __call__(self, prompt_inputs):\n",
    "        client = self.client or OpenAI(api_key=self.api_key)\n",
    "        outputs = []\n",
    "        for item in prompt_inputs:\n",
    "            content = str(item.get(\"agent_query\", \"\"))  # already includes system instruction\n",
    "            resp = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                temperature=self.temperature,\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            )\n",
    "            text = (resp.choices[0].message.content or \"\").strip()\n",
    "            outputs.append({\"text\": text})\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define an Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Variables and a Template\n",
    "import agent_torch.core.llm.template as lm\n",
    "\n",
    "class MyPromptTemplate(lm.Template):\n",
    "    system_prompt = \"Consider the following variables and determine your willingness to isolate.\"\n",
    "    # Variables (choose which are learnable)\n",
    "    salary = lm.Variable(desc=\"salary\")\n",
    "    work_week = lm.Variable(desc=\"hours worked in a week\", learnable=True)\n",
    "    satisfaction = lm.Variable(desc=\"categorical satisfaction levels\", learnable=False, default=\"unknown satisfaction\")\n",
    "    def __prompt__(self):\n",
    "        self.prompt_string = (\n",
    "            \"I work for {work_week} hours, and get paid ${salary}. \"\n",
    "            \"I am {satisfaction} with my current job.\"\n",
    "        )\n",
    "    def __output__(self):\n",
    "        return \"Answer on a scale from 0 - 1.0.\"\n",
    "\n",
    "prompt_template = MyPromptTemplate()\n",
    "arch = Archetype(prompt=prompt_template, llm=llm, n_arch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Data for Variable objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Configure external data and preview\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataframe (from index.md)\n",
    "df = pd.DataFrame({\n",
    "    \"Salary\": [65000, 43000, 120000],\n",
    "    \"Work_Week_Hours\": [60, 40, 80],\n",
    "    \"Satisfaction_Level\": [\"Not satisfied\", \"Partially satisfied\", \"Very satisfied\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compare performance between different Configurations of Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Broadcast to a population and sample decisions\n",
    "arch.configure(external_df=df, split = 2) #configure with only 2 rows\n",
    "arch.sample()\n",
    "\n",
    "from agent_torch.populations import astoria\n",
    "arch.configure(external_df=df) # configure with full dataframe\n",
    "arch.broadcast(population=astoria) # bind population\n",
    "arch.sample() # returns (n_agents,) tensor of decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
