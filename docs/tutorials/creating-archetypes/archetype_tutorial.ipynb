{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Archetypes (New API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Setup\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_torch.core.llm.archetype import Archetype\n",
    "from agent_torch.core.llm.mock_llm import MockLLM\n",
    "import agent_torch.populations.astoria as astoria\n",
    "import pandas as pd, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup : Covid Cases Data and Unemployment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_covid_cases_data\n",
    "\n",
    "csv_path = \"/models/covid/data/county_data.csv\"\n",
    "monthly_cases_kings = get_covid_cases_data(\n",
    "    csv_path=csv_path, county_name=\"Kings County\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Initialise LLM Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use either of the Langchain and Dspy backends to initialise a LLM instance. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use Langchain to initialise an LLM instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n",
    "llm_langchain_35 = LangchainLLM(\n",
    "    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define an Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API: build Template and Archetype\n",
    "import agent_torch.core.llm.template as lm\n",
    "\n",
    "class MyPromptTemplate(lm.Template):\n",
    "    system_prompt = \"You are evaluating willingness based on job profile and context.\"\n",
    "    age = lm.Variable(desc=\"agent age\", learnable=True)\n",
    "    gender = lm.Variable(desc=\"agent gender\", learnable=False)\n",
    "    soc_code = lm.Variable(desc=\"job id\", learnable=False)\n",
    "    abilities = lm.Variable(desc=\"abilities required\", learnable=True)\n",
    "    work_context = lm.Variable(desc=\"work context\", learnable=True)\n",
    "    def __prompt__(self):\n",
    "        self.prompt_string = (\n",
    "            \"You are in your {age}'s and are a {gender}. \"\n",
    "            \"As a {soc_code}...you have {abilities} and work in {work_context}.\"\n",
    "        )\n",
    "    def __output__(self):\n",
    "        return \"Rate your willingness to continue normal activities, respond in [0, 1] binary decision only.\"\n",
    "\n",
    "prompt_template = MyPromptTemplate()\n",
    "llm = MockLLM()\n",
    "arch = Archetype(prompt=prompt_template, llm=llm, n_arch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object of the Behavior class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure external data and preview\n",
    "base_dir = os.path.dirname(__file__)\n",
    "jobs_df = pd.read_pickle(os.path.join(base_dir, \"../../..\", \"job_data_clean.pkl\"))\n",
    "arch.configure(external_df=jobs_df, split=2)\n",
    "arch.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments to be used for creating a query for the LLM Instance\n",
    "kwargs = {\n",
    "    \"month\": \"January\",\n",
    "    \"year\": \"2020\",\n",
    "    \"covid_cases\": 1200,\n",
    "    \"device\": \"cpu\",\n",
    "    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n",
    "    \"unemployment_rate\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compare performance between different Configurations of Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast to population and sample\n",
    "arch.configure(external_df=jobs_df)\n",
    "arch.broadcast(population=astoria, match_on=\"soc_code\")\n",
    "arch.sample(print_examples=1)\n",
    "\n",
    "# Optional: Optimize with P3O\n",
    "from agent_torch.optim import P3O\n",
    "opt = P3O(arch.parameters(), archetype=arch)\n",
    "for _ in range(2):\n",
    "    arch.sample(print_examples=1)\n",
    "    opt.step()\n",
    "    opt.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
