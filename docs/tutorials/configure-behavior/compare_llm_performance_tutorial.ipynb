{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring LLM Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "AgentTorch is a framework for building and deploying AI agents. One crucial component of these agents is the Language Model (LLM) they use. This tutorial will guide you through the process of experimenting with different LLMs in AgentTorch to understand their impact on the framework's effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Setup\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dspy_modules import COT, BasicQAWillToWork\n",
    "from agent_torch.core.llm.archetype import Archetype\n",
    "from agent_torch.core.llm.behavior import Behavior\n",
    "from agent_torch.populations import NYC\n",
    "from agent_torch.core.llm.backend import DspyLLM, LangchainLLM\n",
    "\n",
    "import torch\n",
    "OPENAI_API_KEY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup : Covid Cases Data and Unemployment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_covid_cases_data\n",
    "csv_path = 'agent_torch/models/covid/data/county_data.csv'\n",
    "monthly_cases_kings = get_covid_cases_data(csv_path=csv_path,county_name='Kings County')\n",
    "monthly_cases_queens = get_covid_cases_data(csv_path=csv_path,county_name='Queens County')\n",
    "monthly_cases_bronx = get_covid_cases_data(csv_path=csv_path,county_name='Bronx County')\n",
    "monthly_cases_new_york = get_covid_cases_data(csv_path=csv_path,county_name='New York County')\n",
    "monthly_cases_richmond = get_covid_cases_data(csv_path=csv_path,county_name='Richmond County')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Initialise Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use either of the Langchain and Dspy backends to initialise a LLM Agent. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use Langchain to initialise an LLM Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n",
    "llm_langchain_35 = LangchainLLM(\n",
    "    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 4-0 Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n",
    "llm_langchain_4o_mini = LangchainLLM(\n",
    "    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-4o-mini\"\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly if we wanted to use Dspy backend, we can instantiate the DspyLLM object.\n",
    "We can pass the desired model name as argument just like we did with Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent profile is decided by the QA module and the COT module enforces Chain of Thought reasoning\n",
    "llm_dspy = DspyLLM(qa=BasicQAWillToWork, cot=COT, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define agent Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object of the Behavior class\n",
    "You have options to pass any of the above created llm objects to the behavior class\n",
    "Specify the region for which the behavior is to be generated. This should be the name of any of the regions available in the populations folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object of the Archetype class\n",
    "# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\n",
    "archetype = Archetype(n_arch=2) \n",
    "\n",
    "# Define a prompt template\n",
    "# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n",
    "# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\n",
    "user_prompt_template = \"Your age is {age}, gender is {gender}, ethnicity is {ethnicity}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n",
    "\n",
    "# Create a behavior model\n",
    "# You have options to pass any of the above created llm objects to the behavior class\n",
    "# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\n",
    "earning_behavior_4o_mini = Behavior(\n",
    "    archetype=archetype.llm(llm=llm_langchain_4o_mini, user_prompt=user_prompt_template),\n",
    "    region=NYC\n",
    ")\n",
    "earning_behavior_35 = Behavior(\n",
    "    archetype=archetype.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n",
    "    region=NYC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments to be used for creating a query for the Agent\n",
    "kwargs = {\n",
    "    \"month\": \"January\",\n",
    "    \"year\": \"2020\",\n",
    "    \"covid_cases\": 1200,\n",
    "    \"device\": \"cpu\",\n",
    "    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n",
    "    \"unemployment_rate\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compare performance between different LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labor_data, get_labor_force_correlation\n",
    "\n",
    "labor_force_df_4o_mini, observed_labor_force_4o_mini, correlation_4o_mini = get_labor_force_correlation(\n",
    "    monthly_cases_kings, \n",
    "    earning_behavior_4o_mini, \n",
    "    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n",
    "    kwargs\n",
    ")\n",
    "labor_force_df_35, observed_labor_force_35, correlation_35 = get_labor_force_correlation(\n",
    "    monthly_cases_kings, \n",
    "    earning_behavior_35, \n",
    "    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n",
    "    kwargs\n",
    ")\n",
    "print(f\"Correlation with GPT 3.5 is {correlation_35} and with GPT 4o Mini is {correlation_4o_mini}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
