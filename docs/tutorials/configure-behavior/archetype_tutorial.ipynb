{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archetype Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Setup\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_torch.core.llm.archetype import Archetype\n",
    "from agent_torch.core.llm.behavior import Behavior\n",
    "from agent_torch.populations import NYC\n",
    "from agent_torch.core.llm.backend import LangchainLLM\n",
    "OPENAI_API_KEY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup : Covid Cases Data and Unemployment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_covid_cases_data\n",
    "csv_path = '/models/covid/data/county_data.csv'\n",
    "monthly_cases_kings = get_covid_cases_data(csv_path=csv_path,county_name='Kings County')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Initialise Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use either of the Langchain and Dspy backends to initialise a LLM Agent. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use Langchain to initialise an LLM Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n",
    "llm_langchain_35 = LangchainLLM(\n",
    "    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define an Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object of the Archetype class\n",
    "# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\n",
    "archetype_n_2 = Archetype(n_arch=2) \n",
    "archetype_n_12 = Archetype(n_arch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object of the Behavior class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template\n",
    "# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n",
    "# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\n",
    "user_prompt_template = \"Your age is {age}, gender is {gender}, ethnicity is {ethnicity}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n",
    "\n",
    "# Create a behavior model\n",
    "# You have options to pass any of the above created llm objects to the behavior class\n",
    "# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\n",
    "earning_behavior_n_2 = Behavior(\n",
    "    archetype=archetype_n_2.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n",
    "    region=NYC\n",
    ")\n",
    "earning_behavior_n_12 = Behavior(\n",
    "    archetype=archetype_n_12.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n",
    "    region=NYC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments to be used for creating a query for the Agent\n",
    "kwargs = {\n",
    "    \"month\": \"January\",\n",
    "    \"year\": \"2020\",\n",
    "    \"covid_cases\": 1200,\n",
    "    \"device\": \"cpu\",\n",
    "    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n",
    "    \"unemployment_rate\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compare performance between different Configurations of Archetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_labor_data, get_labor_force_correlation\n",
    "\n",
    "labor_force_df_n_2, observed_labor_force_n_2, correlation_n_2 = get_labor_force_correlation(\n",
    "    monthly_cases_kings, \n",
    "    earning_behavior_n_2, \n",
    "    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n",
    "    kwargs\n",
    ")\n",
    "labor_force_df_n_12, observed_labor_force_n_12, correlation_n_12 = get_labor_force_correlation(\n",
    "    monthly_cases_kings, \n",
    "    earning_behavior_n_12, \n",
    "    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n",
    "    kwargs\n",
    ")\n",
    "print(f\"Correlation with 2 Archetypes is {correlation_n_2} and 12 Archetypes is {correlation_n_12}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
