{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'\n",
    "import sys\n",
    "sys.path.append('/Users/shashankkumar/Documents/GitHub/MacroEcon/AgentTorch')\n",
    "from AgentTorch.LLM.llm_qa import DocumentRetriever\n",
    "import dspy\n",
    "from typing import List, Union, Optional\n",
    "from dsp.utils import deduplicate\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandasai import SmartDataframe\n",
    "from pandasai.llm import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "directory = \"/Users/shashankkumar/Documents/GitHub/MacroEcon/simulator_data/census_populations/NYC/simulation_memory_output\"\n",
    "search_kwargs = {\"k\": 5}\n",
    "# document_retriever = DocumentRetriever(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs,directory=directory, search_kwargs=search_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Query Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_runner = QueryRunner(retriever=document_retriever.retriever,openai_api_key=OPENAI_API_KEY)\n",
    "query_runner(query=\"What is the approximate consumption rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dspy Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AgentTorch.LLM.llm_qa import DSPythonicRMClient, QueryRunnerUsingDspy\n",
    "\n",
    "rm = DSPythonicRMClient(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs,directory=directory,search_kwargs=search_kwargs)\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', api_key=OPENAI_API_KEY)\n",
    "dspy.settings.configure(lm=turbo, rm=rm)\n",
    "# Ask any question you like to this simple RAG program.\n",
    "my_question = \"How many age groups are there in the population?\"\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "query_runner = QueryRunnerUsingDspy()  # uncompiled (i.e., zero-shot) program\n",
    "pred = query_runner(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = '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'\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "directory = \"/Users/shashankkumar/Documents/GitHub/MacroEcon/simulator_data/census_populations/NYC/simulation_memory_output\"\n",
    "search_kwargs = {\"k\": 5}\n",
    "document_retriever = DocumentRetriever(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs,directory=directory, search_kwargs=search_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4\")\n",
    "\n",
    "system_prompt = \"\"\"You are a data analyst responsible for breaking down complex queries into smaller, more manageable subqueries. Your task is to take a given query and split it into a series of subqueries, each building upon the previous one(s), except for the last subquery, which should use the results from the previous subqueries to generate the final answer.\n",
    "\n",
    "For each subquery, provide a clear and concise description of the data or information needed, without including any code. These descriptions will be used by other agents to generate the appropriate Pandas code for retrieving the required data.\n",
    "\n",
    "Your output should be a JSON object with keys representing each subquery, and the corresponding values being the descriptions for those subqueries. The number of subqueries can vary depending on the complexity of the original query, so you should dynamically create the appropriate number of keys in the JSON object.\n",
    "\n",
    "For example, if you have three subqueries, your JSON output should look like this:\n",
    "\n",
    "{{\n",
    "  \"subquery1\": \"Description for subquery 1\",\n",
    "  \"subquery2\": \"Description for subquery 2\",\n",
    "  \"subquery3\": \"Description for subquery 3\"\n",
    "}}\n",
    "\n",
    "Remember, your goal is to provide clear and concise descriptions for each subquery, without including any code. The generated JSON object will be used by other agents to retrieve the necessary data and ultimately generate the final answer to the original query.\"\"\"\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create a processing chain with the prompt template and the OpenAI chat model\n",
    "processing_chain = prompt_template | openai_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = processing_chain.invoke({\"input\": \"What is the approximate consumption rate for old aged people in the population?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_chain.invoke({\"input\": \"What is the consumption trend for females?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/shashankkumar/Documents/GitHub/MacroEcon/state_data_dict.pkl', 'rb') as handle:\n",
    "    sim_data_dict = pickle.load(handle)\n",
    "agent_prop = sim_data_dict['agents']\n",
    "env_prop = sim_data_dict['environment']\n",
    "processed_data = {'consumers':{}}\n",
    "for key, value in agent_prop['consumers'].items():\n",
    "    value = value.flatten().squeeze()\n",
    "    processed_data['consumers'][key] = value.numpy()\n",
    "processed_data['consumers']['assets'] = processed_data['consumers']['assets'][:37518]\n",
    "df = pd.DataFrame(processed_data['consumers'])\n",
    "# Flatten the nested structure\n",
    "df = df.explode(['assets', 'consumption_propensity', 'monthly_consumption', 'monthly_income', 'post_tax_income', 'will_work', 'work_propensity'])\n",
    "# Ensure consistent data types\n",
    "df = df.astype({'ID': int, 'age': float, 'area': float, 'assets': float, 'consumption_propensity': float, 'ethnicity': float, 'gender': float, 'monthly_consumption': float, 'monthly_income': float, 'post_tax_income': float, 'will_work': float, 'work_propensity': float})\n",
    "openai_llm = OpenAI(\n",
    "    api_token=OPENAI_API_KEY,\n",
    ")\n",
    "dfs = SmartDataframe(df,config={\"llm\": openai_llm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import Agent\n",
    "\n",
    "pandas_agent = Agent(df,config={\"llm\": openai_llm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in out:\n",
    "    print(agent.chat(agent.rephrase_query(out[query])),query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"What is the income for ethnic groups?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "@tool\n",
    "def run_analysis_on_simulation_state(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the analysis on the simulation state\n",
    "    Use this tool to generate Pandas code for data retrieval.\n",
    "    :param query: The query to run\n",
    "    :return: The data analysis result\n",
    "    \"\"\"\n",
    "    return pandas_agent.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eec9fc4ce5448378efa91f3772a63d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = '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'\n",
    "\n",
    "# Define the system prompt for the OpenAI chat model\n",
    "system_prompt = \"\"\"You are a data analyst responsible for breaking down complex queries into smaller, more manageable subqueries. Your task is to take a given query and split it into a series of subqueries, each building upon the previous one(s), except for the last subquery, which should use the results from the previous subqueries to generate the final answer.\n",
    "\n",
    "For each subquery, provide a clear and concise description of the data or information needed, without including any code. These descriptions will be used by other agents to generate the appropriate Pandas code for retrieving the required data.\n",
    "\n",
    "Your output should be a JSON object with keys representing each subquery, and the corresponding values being the descriptions for those subqueries. The number of subqueries can vary depending on the complexity of the original query, so you should dynamically create the appropriate number of keys in the JSON object.\n",
    "\n",
    "For example, if you have three subqueries, your JSON output should look like this:\n",
    "\n",
    "{\n",
    "\"subquery1\": \"Description for subquery 1\",\n",
    "\"subquery2\": \"Description for subquery 2\",\n",
    "\"subquery3\": \"Description for subquery 3\"\n",
    "}\n",
    "\n",
    "Remember, your goal is to provide clear and concise descriptions for each subquery, without including any code. The generated JSON object will be used by other agents to retrieve the necessary data and ultimately generate the final answer to the original query.\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "openai_chat_model = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Initialize the PandasAI tool\n",
    "pandas_agent = Agent(df,config={\"llm\": openai_llm})\n",
    "from langchain.tools import tool\n",
    "@tool\n",
    "def run_analysis_on_simulation_state(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Run the analysis on the simulation state\n",
    "    Use this tool to generate Pandas code for data retrieval.\n",
    "    :param query: The query to run\n",
    "    :return: The data analysis result\n",
    "    \"\"\"\n",
    "    return pandas_agent.chat(query)\n",
    "\n",
    "document_retriever = DocumentRetriever(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs,directory=directory, search_kwargs=search_kwargs)\n",
    "retriever_tool = create_retriever_tool(\n",
    "    document_retriever.retriever,\n",
    "    \"simulation_memory_retriever\",\n",
    "    \"You must use this tool to retrive context for each user query!\",\n",
    ")\n",
    "# Initialize the LangChain agent\n",
    "tools = [run_analysis_on_simulation_state, retriever_tool]\n",
    "\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "llm_agent = create_tool_calling_agent(openai_chat_model, tools, prompt)\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=llm_agent, tools=tools, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `run_analysis_on_simulation_state` with `{'query': 'income by ethnic groups'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m   ethnicity  monthly_income\n",
      "0        0.0      313.654739\n",
      "1        1.0      313.221268\n",
      "2        2.0      314.367449\n",
      "3        3.0      314.094120\n",
      "4        4.0      313.727248\n",
      "5        5.0      314.069188\u001b[0m\u001b[32;1m\u001b[1;3mThe income distribution among different ethnic groups appears to be quite equitable. The monthly income for all groups is around 313-314 units. This suggests that there is no significant income disparity based on ethnicity in the current simulation state. However, it's important to note that this is a simplified representation and real-world scenarios may be more complex and influenced by various other factors.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the income for ethnic groups? Give your comment on it.',\n",
       " 'output': \"The income distribution among different ethnic groups appears to be quite equitable. The monthly income for all groups is around 313-314 units. This suggests that there is no significant income disparity based on ethnicity in the current simulation state. However, it's important to note that this is a simplified representation and real-world scenarios may be more complex and influenced by various other factors.\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What is the income for ethnic groups? Give your comment on it.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `simulation_memory_retriever` with `{'query': 'consumption rate for old aged people'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f08cda0fcdf4624ab427d8293316cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3mspend on essential goods).'), AIMessage(content='{\\n  \"work\": 0.85,\\n  \"consumption\": 0.60\\n}'), HumanMessage(content='You are male of age 65A. With all these factors in play, and considering aspects like your living costs, any future aspirations, and the broader economic trends, how is your\n",
      "\n",
      "intervals of 0.02, indicating the proportion of all your savings and income you intend to spend on essential goods).'), AIMessage(content='{\\n  \"work\": 0.80,\\n  \"consumption\": 0.70\\n}'), HumanMessage(content='You are male of age 65A. With all these factors in play, and considering aspects like your\n",
      "\n",
      "intervals of 0.02, indicating the proportion of all your savings and income you intend to spend on essential goods).'), AIMessage(content='{\\n  \"work\": 0.85,\\n  \"consumption\": 0.60\\n}'), HumanMessage(content='You are male of age 65A. With all these factors in play, and considering aspects like your\n",
      "\n",
      "intervals of 0.02, indicating the proportion of all your savings and income you intend to spend on essential goods).'), AIMessage(content='{\\n  \"work\": 0.57,\\n  \"consumption\": 0.70\\n}'), HumanMessage(content='You are female of age 65A. With all these factors in play, and considering aspects like\n",
      "\n",
      "\"work\": 0.89,\\n  \"consumption\": 0.65\\n}')]}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `run_analysis_on_simulation_state` with `{'query': 'consumption rate for old aged people'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: RuntimeWarning: invalid value encountered in scalar divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mnan\u001b[0m\u001b[32;1m\u001b[1;3mI'm sorry, but it seems there is an issue with the data retrieval. I am unable to provide the approximate consumption rate for old aged people in the population at the moment. However, based on the previous data, it appears that the consumption rate varies between 0.60 and 0.70. This suggests that older individuals are spending a significant portion of their income on essential goods. This could be due to higher healthcare costs or other necessities. It's important to ensure that social systems and policies are in place to support this demographic, especially considering the increasing aging population.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the approximate consumption rate for old aged people in the population? Give your comment on it.',\n",
       " 'output': \"I'm sorry, but it seems there is an issue with the data retrieval. I am unable to provide the approximate consumption rate for old aged people in the population at the moment. However, based on the previous data, it appears that the consumption rate varies between 0.60 and 0.70. This suggests that older individuals are spending a significant portion of their income on essential goods. This could be due to higher healthcare costs or other necessities. It's important to ensure that social systems and policies are in place to support this demographic, especially considering the increasing aging population.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What is the approximate consumption rate for old aged people in the population? Give your comment on it.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `run_analysis_on_simulation_state` with `{'query': 'consumption trend for different age groups'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m   age  monthly_consumption\n",
      "0  0.0                  0.0\n",
      "1  1.0                  0.0\n",
      "2  2.0                  0.0\n",
      "3  3.0                  0.0\n",
      "4  4.0                  0.0\n",
      "5  5.0                  0.0\u001b[0m\u001b[32;1m\u001b[1;3mThe consumption trend for different age groups is as follows:\n",
      "\n",
      "- Age 0: 0.0\n",
      "- Age 1: 0.0\n",
      "- Age 2: 0.0\n",
      "- Age 3: 0.0\n",
      "- Age 4: 0.0\n",
      "- Age 5: 0.0\n",
      "\n",
      "It appears that there is no consumption for the age groups from 0 to 5.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the consumption trend for different age groups?',\n",
       " 'output': 'The consumption trend for different age groups is as follows:\\n\\n- Age 0: 0.0\\n- Age 1: 0.0\\n- Age 2: 0.0\\n- Age 3: 0.0\\n- Age 4: 0.0\\n- Age 5: 0.0\\n\\nIt appears that there is no consumption for the age groups from 0 to 5.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"What is the consumption trend for different age groups?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `simulation_memory_retriever` with `{'query': 'consumption'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709bc1f1e8d7487a931e5aca97fa0597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33;1m\u001b[1;3m\"work\": 0.88,\\n  \"consumption\": 0.63\\n}')]}\n",
      "\n",
      "\"work\": 0.89,\\n  \"consumption\": 0.65\\n}')]}\n",
      "\n",
      "\"work\": 0.84,\\n  \"consumption\": 0.68\\n}')]}\n",
      "\n",
      "\"work\": 0.88,\\n  \"consumption\": 0.68\\n}')]}\n",
      "\n",
      "\"work\": 0.87,\\n  \"consumption\": 0.67\\n}')]}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `run_analysis_on_simulation_state` with `{'query': 'consumption with extra income of 500 usd'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/shashankkumar/anaconda3/envs/LIDA/lib/python3.10/site-packages/pandasai/pipelines/chat/generate_chat_pipeline.py\", line 283, in run\n",
      "    output = (self.code_generation_pipeline | self.code_execution_pipeline).run(\n",
      "  File \"/Users/shashankkumar/anaconda3/envs/LIDA/lib/python3.10/site-packages/pandasai/pipelines/pipeline.py\", line 137, in run\n",
      "    raise e\n",
      "  File \"/Users/shashankkumar/anaconda3/envs/LIDA/lib/python3.10/site-packages/pandasai/pipelines/pipeline.py\", line 101, in run\n",
      "    step_output = logic.execute(\n",
      "  File \"/Users/shashankkumar/anaconda3/envs/LIDA/lib/python3.10/site-packages/pandasai/pipelines/chat/code_execution.py\", line 114, in execute\n",
      "    raise e\n",
      "  File \"/Users/shashankkumar/anaconda3/envs/LIDA/lib/python3.10/site-packages/pandasai/pipelines/chat/code_execution.py\", line 98, in execute\n",
      "    raise InvalidOutputValueMismatch(\n",
      "pandasai.exceptions.InvalidOutputValueMismatch: Value type <class 'list'> must match with type dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mUnfortunately, I was not able to answer your question, because of the following error:\n",
      "\n",
      "Value type <class 'list'> must match with type dataframe\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mI'm sorry, but I'm unable to provide a direct answer to your question as it requires a specific simulation to be run with the mentioned conditions. However, generally speaking, an increase in income, such as an extra $500, would likely lead to an increase in consumption as people would have more disposable income to spend. This is based on the principle of marginal propensity to consume, which states that as income increases, consumption also increases.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How would have consumtpion changed if governement had provided extra income of 500 usd?',\n",
       " 'output': \"I'm sorry, but I'm unable to provide a direct answer to your question as it requires a specific simulation to be run with the mentioned conditions. However, generally speaking, an increase in income, such as an extra $500, would likely lead to an increase in consumption as people would have more disposable income to spend. This is based on the principle of marginal propensity to consume, which states that as income increases, consumption also increases.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"How would have consumtpion changed if governement had provided extra income of 500 usd?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subqueries(query):\n",
    "    # Get the response from the OpenAI chat model\n",
    "    response = openai_chat_model(prompt_template.format_prompt(input=query).to_messages())\n",
    "\n",
    "    # Parse the response as JSON\n",
    "    try:\n",
    "        subqueries = json.loads(response.content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON response. Please try again.\")\n",
    "        return None\n",
    "\n",
    "    return subqueries\n",
    "\n",
    "def process_subqueries(subqueries, data):\n",
    "    results = []\n",
    "    for key, value in subqueries.items():\n",
    "        # Generate Pandas code using PandasAI\n",
    "        pandas_code = llm_agent.run(f\"Generate Pandas code to {value} from the given data.\")\n",
    "        print(f\"Pandas code for {key}: {pandas_code}\")\n",
    "\n",
    "        # Execute the Pandas code and store the result\n",
    "        result = eval(pandas_code)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the average age of employees in the sales department who have been with the company for more than 5 years?\"\n",
    "\n",
    "# Load sample data\n",
    "data = pd.read_csv(\"employees.csv\")\n",
    "\n",
    "# Generate subqueries\n",
    "subqueries = generate_subqueries(query)\n",
    "\n",
    "if subqueries:\n",
    "    # Process subqueries and generate Pandas code for data retrieval\n",
    "    results = process_subqueries(subqueries, data)\n",
    "\n",
    "    # Code to combine the results from subqueries and generate the final answer goes here\n",
    "    # ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
