{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "import glob\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "OPENAI_API_KEY = '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'\n",
    "import sys\n",
    "sys.path.append('/Users/shashankkumar/Documents/GitHub/MacroEcon/AgentTorch')\n",
    "from AgentTorch.LLM.llm_qa import DocumentRetriever,QueryRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shashankkumar/anaconda3/envs/LIDA/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the trend in consumption accross age', 'result': \" I don't know.\"}\n",
      "{'query': 'tell me something about the data', 'result': '\\nThe data appears to be a conversation between a human and an AI, discussing the willingness to work and planned expenditures on essential goods for a female under the age of 19. The human is asked to share their decisions in a JSON format, with two keys for \"work\" and \"consumption\" and values for each in the range of 0 to 1 with intervals of 0.02. The AI responds with values for each key based on the human\\'s decisions. This conversation is repeated multiple times, likely to gather different perspectives or decisions.'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "directory = \"/Users/shashankkumar/Documents/GitHub/MacroEcon/simulator_data/census_populations/NYC/simulation_input/simulation_memory_output/1/12\"\n",
    "document_retriever = DocumentRetriever(model_name, model_kwargs, encode_kwargs, directory)\n",
    "query_runner = QueryRunner(retriever=document_retriever.retriever,openai_api_key=OPENAI_API_KEY)\n",
    "query_processor = query_runner(query=\"What is the population of NYC in 2020?\")\n",
    "query_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LIDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
