{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Large Population Models    <p> making complexity simple   differentiable learning over millions of autonomous agents </p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Many grand challenges like climate change and pandemics emerge from complex interactions of millions of individual decisions. While LLMs and AI agents excel at individual behavior, they can't model these intricate societal dynamics. Enter Large Population Models LPMs: a new AI paradigm simulating millions of interacting agents simultaneously, capturing collective behaviors at societal scale. It's like scaling up AI agents exponentially to understand the ripple effects of countless decisions.</p> <p>AgentTorch, our open-source platform, makes building and running these massive simulations accessible. It's optimized for GPUs, allowing efficient simulation of entire cities or countries. Think PyTorch, but for large-scale agent-based simulations. AgentTorch LPMs have four design principles:</p> <ul> <li>Scalability: AgentTorch models can simulate country-size populations in   seconds on commodity hardware.</li> <li>Differentiability: AgentTorch models can differentiate through simulations   with stochastic dynamics and conditional interventions, enabling   gradient-based learning.</li> <li>Composition: AgentTorch models can compose with deep neural networks (eg:   LLMs), mechanistic simulators (eg: mitsuba) or other LPMs. This helps describe   agent behavior using LLMs, calibrate simulation parameters and specify   expressive interaction rules.</li> <li>Generalization: AgentTorch helps simulate diverse ecosystems - humans in   geospatial worlds, cells in anatomical worlds, autonomous avatars in digital   worlds.</li> </ul> <p>LPMs are already making real-world impact. They're being used to help immunize millions of people by optimizing vaccine distribution strategies, and to track billions of dollars in global supply chains, improving efficiency and reducing waste. Our long-term goal is to \"re-invent the census\": built entirely in simulation, captured passively and used to protect country-scale populations. Our research is early but actively making an impact - winning awards at AI conferences and being deployed across the world. Learn more about LPMs here.</p> <p>AgentTorch is building the future of decision engines - inside the body, around us and beyond!</p> <p>https://github.com/AgentTorch/AgentTorch/assets/13482350/4c3f9fa9-8bce-4ddb-907c-3ee4d62e7148</p>"},{"location":"#installation","title":"Installation","text":"<p>The easiest way to install AgentTorch (v0.4.0) is from pypi:</p> <pre><code>&gt; pip install agent-torch\n</code></pre> <p>AgentTorch is meant to be used in a Python 3.9 environment. If you have not installed Python 3.9, please do so first from python.org/downloads.</p> <p>Install the most recent version from source using <code>pip</code>:</p> <pre><code>&gt; pip install git+https://github.com/agenttorch/agenttorch\n</code></pre> <p>Some models require extra dependencies that have to be installed separately. For more information regarding this, as well as the hardware the project has been run on, please see <code>docs/install.md</code>.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The following section depicts the usage of existing models and population data to run simulations on your machine. It also acts as a showcase of the Agent Torch API.</p> <p>A Jupyter Notebook containing the below examples can be found here.</p>"},{"location":"#executing-a-simulation","title":"Executing a Simulation","text":"<pre><code># re-use existing models and population data easily\nfrom agent_torch.models import covid\nfrom agent_torch.populations import astoria\n\n# use the executor to plug-n-play\nfrom agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\n\n# agent_\"torch\" works seamlessly with the pytorch API\nfrom torch.optim import SGD\n\nloader = LoadPopulation(astoria)\nsimulation = Executor(model=covid, pop_loader=loader)\n\nsimulation.init(SGD)\nsimulation.execute()\n</code></pre>"},{"location":"#guides-and-tutorials","title":"Guides and Tutorials","text":""},{"location":"#understanding-the-framework","title":"Understanding the Framework","text":"<p>A detailed explanation of the architecture of the Agent Torch framework can be found here.</p>"},{"location":"#building-simulations-with-the-configuration-api","title":"Building Simulations with the Configuration API","text":"<p>Learn how to create and customize agent-based simulations using AgentTorch's powerful Configuration API. This tutorial walks you through: - Creating agents with custom properties - Defining environment variables and networks - Building simulation substeps with policies and transitions - Best practices for organizing your simulation</p> <p>Get started with the Config API tutorial \u2192</p>"},{"location":"#optimizing-performance-with-vectorized-operations","title":"Optimizing Performance with Vectorized Operations","text":"<p>Learn how to leverage AgentTorch's vectorized operations for high-performance simulations: - Understanding vectorized vs standard operations - Converting standard functions to vectorized implementations - Using batched processing for large populations - Performance optimization techniques</p> <p>Learn about vectorized operations \u2192</p>"},{"location":"#creating-a-model","title":"Creating a Model","text":"<p>A tutorial on how to create a simple predator-prey model can be found in the <code>tutorials/</code> folder.</p>"},{"location":"#prompting-collective-behavior-with-llm-archetypes","title":"Prompting Collective Behavior with LLM Archetypes","text":"<pre><code>from agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.core.llm.backend import LangchainLLM\nfrom agent_torch.populations import NYC\n\nuser_prompt_template = \"Your age is {age} {gender},{unemployment_rate} the number of COVID cases is {covid_cases}.\"\n\n# Using Langchain to build LLM Agents\nagent_profile = \"You are a person living in NYC. Given some info about you and your surroundings, decide your willingness to work. Give answer as a single number between 0 and 1, only.\"\nllm_langchian = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n)\n\n# Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=7)\n\n# Create an object of the Behavior class\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be generated. This should be the name of any of the regions available in the populations folder.\nearning_behavior = Behavior(\n    archetype=archetype.llm(llm=llm_langchian, user_prompt=user_prompt_template), region=NYC\n)\n\nkwargs = {\n    \"month\": \"January\",\n    \"year\": \"2020\",\n    \"covid_cases\": 1200,\n    \"device\": \"cpu\",\n    \"current_memory_dir\": \"/path-to-save-memory\",\n    \"unemployment_rate\": 0.05,\n}\n\noutput = earning_behavior.sample(kwargs)\n</code></pre>"},{"location":"#contributing-to-agent-torch","title":"Contributing to Agent Torch","text":"<p>Thank you for your interest in contributing! You can contribute by reporting and fixing bugs in the framework or models, working on new features for the framework, creating new models, or by writing documentation for the project.</p> <p>Take a look at the contributing guide for instructions on how to setup your environment, make changes to the codebase, and contribute them back to the project.</p>"},{"location":"#impact","title":"Impact","text":"<p>AgentTorch models are being deployed across the globe.</p> <p></p>"},{"location":"architecture/","title":"Framework Architecture","text":"<p>This document details the architecture of the AgentTorch project, explains all the building blocks involved and points to relevant code implementation and examples.</p> <p>A high-level overview of the AgentTorch Python API is provided by the following block diagram:</p> <p></p> <p>The AgentTorch Python API provides developers with the ability to programmatically create and configure LPMs. This functionality is detailed further in the following sections.</p>"},{"location":"architecture/#runtime","title":"Runtime","text":"<p>The AgentTorch runtime is composed of three essential blocks: the configuration, the registry, and the runner.</p> <p>The configuration holds information about the environment, initial and current state, agents, objects, network metadata, as well as substep definitions. The 'configurator' is defined in <code>config.py</code>.</p> <p>The registry stores all registered substeps, and helper functions, to be called by the runner. It is defined in <code>registry.py</code>.</p> <p>The runner accepts a registry and configuration, and exposes an API to execute all, single or multiple episodes/steps in a simulation. It also maintains the state and trajectory of the simulation across these episodes. It is defined in <code>runner.py</code>, and the substep execution and optimization logic is part of <code>controller.py</code>.</p>"},{"location":"architecture/#data","title":"Data","text":"<p>The data layer is composed of any raw, domain-specific data used by the model (such as agent or object initialization data, environment variables, etc.) as well as the files (YAML or Python code) used to configure the model. An example of domain-specific data for a LPM can be found in the <code>models/covid/data</code> folder. The configuration for the same model can be found in <code>config.yaml</code>.</p>"},{"location":"architecture/#base-classes","title":"Base Classes","text":"<p>The base classes of <code>Agent</code>, <code>Object</code> and <code>Substep</code> form the foundation of the simulation. The agents defined in the configuration learn and interact with either their environment, other agents, or objects through substeps. Substeps are executed in the order of their definition in the configuration, and are split into three parts: <code>SubstepObservation</code>, <code>SubstepAction</code> and <code>SubstepTransition</code>.</p> <ul> <li>A <code>SubstepObservation</code> is defined to observe the state, and pick out those   variables that are of use to the current substep.</li> <li>A <code>SubstepAction</code>, sometimes called a <code>SubstepPolicy</code>, decides the course of   action based on the observations made, and then simulates that action.</li> <li>A <code>SubstepTransition</code> outputs the updates to be made to state variables based   on the action taken in the substep.</li> </ul> <p>An example of a substep will all three parts defined can be found here.</p>"},{"location":"architecture/#domain-extended-classes","title":"Domain Extended Classes","text":"<p>These classes are defined by the developer/user configuring the model, in accordance with the domain of the model. For example, in the COVID model, citizens of the populace are defined as <code>Agents</code>, and <code>Transmission</code> and <code>Quarantine</code> as substeps.</p>"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Thanks for your interest in contributing to Agent Torch! This guide will show you how to set up your environment and contribute to this library.</p>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You must have the following software installed:</p> <ol> <li><code>git</code> (latest)</li> <li><code>python</code> (&gt;= 3.10)</li> </ol> <p>Once you have installed the above, follow these instructions to <code>fork</code> and <code>clone</code> the repository (<code>AgentTorch/AgentTorch</code>).</p> <p>Once you have forked and cloned the repository, you can pick out an issue you want to fix/implement!</p>"},{"location":"contributing/#making-changes","title":"Making Changes","text":"<p>Once you have cloned the repository to your computer (say, in <code>~/Code/AgentTorch</code>) and picked the issue you want to tackle, create a virtual environment, install all dependencies, and create a new branch to hold all your work.</p> <pre><code># create a virtual environment\n&gt; python -m venv .venv/\n\n# set it up\n&gt; . .venv/bin/activate\n&gt; pip install -r development.txt\n&gt; pip install -e .\n\n# set up the pre commit hooks\n&gt; pre-commit install --config pre-commit.yaml\n\n# create a new branch\n&gt; git switch master\n&gt; git switch --create branch-name\n</code></pre> <p>While naming your branch, make sure the name is short and self explanatory.</p> <p>Once you have created a branch, you can start coding!</p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<p>The project is structured as follows. The comments written next to the file/folder give a brief explanation of what purpose the file/folder serves.</p> <pre><code>.\n\u251c\u2500\u2500 agent_torch/\n\u2502  \u251c\u2500\u2500 helpers/ # defines helper functions used to initialize or work with the state of the simulation.\n\u2502  \u251c\u2500\u2500 llm/ # contains all the code related to using llms as agents in the simulation\n\u2502  \u251c\u2500\u2500 __init__.py # exports everything to the world\n\u2502  \u251c\u2500\u2500 config.py # handles reading and processing the simulation's configuration\n\u2502  \u251c\u2500\u2500 controller.py # executes the substeps for each episode\n\u2502  \u251c\u2500\u2500 initializer.py # creates a simulation from a configuration and registry\n\u2502  \u251c\u2500\u2500 registry.py # registry that stores references to the implementations of the substeps and helper functions\n\u2502  \u251c\u2500\u2500 runner.py # executes the episodes of the simulation, and handles its state\n\u2502  \u251c\u2500\u2500 substep.py # contains base classes for the substep observations, actions and transitions\n\u2502  \u2514\u2500\u2500 utils.py # utility functions used throughout the project\n\u251c\u2500\u2500 docs/\n\u2502  \u251c\u2500\u2500 media/ # assets like screenshots or diagrams inserted in .md files\n\u2502  \u251c\u2500\u2500 tutorials/ # jupyter notebooks with tutorials and their explanations\n\u2502  \u251c\u2500\u2500 architecture.md # the framework's architecture\n\u2502  \u2514\u2500\u2500 install.md # instructions on installing the framework\n\u251c\u2500\u2500 models/\n\u2502  \u251c\u2500\u2500 covid/ # a model simulating disease spread, using the example of covid 19\n\u2502  \u2514\u2500\u2500 predator_prey/ # a simple model used to showcase the features of the framework\n\u251c\u2500\u2500 citation.bib # contains the latex code to use to cite this project\n\u251c\u2500\u2500 contributing.md # this file, helps onboard contributors\n\u251c\u2500\u2500 license.md # contains the license for this project (MIT)\n\u251c\u2500\u2500 readme.md # contains details on the what, why, and how\n\u251c\u2500\u2500 requirements.txt # lists the dependencies of the framework\n\u2514\u2500\u2500 setup.py # defines metadata for the project\n</code></pre> <p>Note that after making any code changes, you should run the <code>black</code> code formatter, as follows:</p> <pre><code>&gt; black agent_torch/ tests/\n</code></pre> <p>You should also ensure all the unit tests pass, especially if you have made changes to any files in the <code>agent_torch/</code> folder.</p> <pre><code>&gt; pytest -vvv tests/\n</code></pre> <p>For any changes to the documentation, run <code>prettier</code> over the <code>*.md</code> files after making changes to them. To preview the generated documentation, run:</p> <pre><code>&gt; mkdocs serve\n</code></pre> <p>Rememeber to add any new pages to the sidebar by editing <code>mkdocs.yaml</code>.</p> <p>If you wish to write a tutorial, write it in a Jupyter Notebook, and then convert it to a markdown file using <code>nbconvert</code>:</p> <pre><code>&gt; pip install nbconvert\n&gt; jupyter nbconvert --to markdown &lt;file&gt;.ipynb\n&gt; mv &lt;file&gt;.md index.md\n</code></pre> <p>Rememeber to move any files that it generates to the <code>docs/media</code> folder, and update the hyperlinks in the generated markdown file.</p>"},{"location":"contributing/#saving-changes","title":"Saving Changes","text":"<p>After you have made changes to the code, you will want to <code>commit</code> (basically, Git's version of save) the changes. To commit the changes you have made locally:</p> <pre><code>&gt; git add this/folder that-file.js\n&gt; git commit --message 'commit-message'\n</code></pre> <p>While writing the <code>commit-message</code>, try to follow the below guidelines:</p> <p>Prefix the message with <code>type:</code>, where <code>type</code> is one of the following dependending on what the commit does:</p> <ul> <li><code>fix</code>: Introduces a bug fix.</li> <li><code>feat</code>: Adds a new feature.</li> <li><code>test</code>: Any change related to tests.</li> <li><code>perf</code>: Any performance related change.</li> <li><code>meta</code>: Any change related to the build process, workflows, issue templates,   etc.</li> <li><code>refc</code>: Any refactoring work.</li> <li><code>docs</code>: Any documentation related changes.</li> </ul> <p>Try to keep the first line brief, and less than 60 characters. Describe the change in detail in a new paragraph (double newline after the first line).</p>"},{"location":"contributing/#contributing-changes","title":"Contributing Changes","text":"<p>Once you have committed your changes, you will want to <code>push</code> (basically, publish your changes to GitHub) your commits. To push your changes to your fork:</p> <pre><code>&gt; git push origin branch-name\n</code></pre> <p>If there are changes made to the <code>master</code> branch of the <code>AgentTorch/AgentTorch</code> repository, you may wish to merge those changes into your branch. To do so, you can run the following commands:</p> <pre><code>&gt; git fetch upstream master\n&gt; git merge upstream/master\n</code></pre> <p>This will automatically add the changes from <code>master</code> branch of the <code>AgentTorch/AgentTorch</code> repository to the current branch. If you encounter any merge conflicts, follow this guide to resolve them.</p> <p>Once you have pushed your changes to your fork, follow these instructions to open a <code>pull request</code>:</p> <p>Once you have submitted a pull request, the maintainers of the repository will review your pull requests and provide feedback. If they find the work to be satisfactory, they will merge the pull request.</p>"},{"location":"contributing/#thanks-for-contributing","title":"Thanks for contributing!","text":""},{"location":"install/","title":"Installation Guide","text":"<p>AgentTorch is meant to be used in a Python 3.9 environment (or above). If you have not installed Python 3.9, please do so first from python.org/downloads.</p> <p>To install the project, run:</p> <pre><code>&gt; pip install git+https://github.com/agenttorch/agenttorch\n</code></pre> <p>To run some models, you may need to separately install their dependencies. These usually include <code>torch</code>, <code>torch_geometric</code>, and <code>osmnx</code>.</p> <p>For the sake of completeness, a summary of the commands required is given below:</p> <pre><code># on macos, cuda is not available:\n&gt; pip install torch torchvision torchaudio\n&gt; pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv\n&gt; pip install osmnx\n\n# on ubuntu, where ${CUDA} is the cuda version:\n&gt; pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/${CUDA}\n&gt; pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+${CUDA}.html\n&gt; pip install osmnx\n</code></pre>"},{"location":"install/#hardware","title":"Hardware","text":"<p>The code has been tested on macOS Catalina 10.1.7 and Ubuntu 22.04.2 LTS. Large-scale experiments are run using Nvidia's TitanX and V100 GPUs.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>The following tutorials, in alphabetical order, can be found in this folder:</p> <ul> <li>Creating a simulation</li> <li>Building simulations with Config API</li> <li>Gradient-based calibration</li> <li>Inserting a population</li> <li>From simulation to reality</li> <li>Prompting LLM as LPM Agents</li> </ul>"},{"location":"tutorials/example_v2_optimizations/","title":"Example v2 optimizations","text":""},{"location":"tutorials/example_v2_optimizations/#example-v2-runtime-optimizations-explained","title":"Example v2: Runtime optimizations explained","text":"<p>This note documents the performance-minded choices used in <code>example_v2.py</code>, why they help, and how they reduce end\u2011to\u2011end time.</p>"},{"location":"tutorials/example_v2_optimizations/#1-auto-device-selection-cpu-vs-cuda","title":"1) Auto device selection (CPU vs CUDA)","text":"<ul> <li>What: The <code>Executor</code> auto-detects CUDA and instantiates the appropriate runner.</li> <li>Why it helps: Utilizes GPU kernels for heavy tensor work without manual configuration.</li> <li>How it helps: Large state updates and transitions run on the GPU where available, improving throughput.</li> </ul> <p>```100:106:example_v2.py     runner = setup(covid, astoria)     learn_params = [(name, params) for (name, params) in runner.named_parameters()]     # Ensure new tensor is on the same device as the runner     device = runner.device</p> <pre><code>\n### 2) Minimize host\u2194device transfers (create tensors on runner.device)\n\n- What: New tensors (e.g., learnable parameters) are created directly on `runner.device`.\n- Why it helps: Avoids implicit CPU\u2192GPU copies that add latency and can fragment GPU memory.\n- How it helps: Keeps data resident on the device where compute happens.\n\n```105:113:example_v2.py\n    device = runner.device\n    new_tensor = torch.tensor([3.5, 4.2, 5.6], requires_grad=True, device=device)\n    input_string = learn_params[0][0]\n    input_string = \"initializer.transition_function.0.new_transmission.learnable_args.R2\"\n    params_dict = {input_string: new_tensor}\n    runner._set_parameters(params_dict)\n</code></pre>"},{"location":"tutorials/example_v2_optimizations/#3-batch-stepping-reduce-python-overhead","title":"3) Batch stepping (reduce Python overhead)","text":"<ul> <li>What: Call <code>runner.step(num_steps_per_episode)</code> once instead of stepping inside a Python loop.</li> <li>Why it helps: Minimizes Python overhead and lets the runner fuse/sequence operations internally.</li> <li>How it helps: The inner loop runs in optimized code paths (vectorized kernels / graph where applicable).</li> </ul> <p>```79:88:example_v2.py def simulate(runner):     num_steps_per_episode = runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"]     runner.step(num_steps_per_episode)     traj = runner.state_trajectory[-1][-1]     preds = traj[\"environment\"][\"daily_infected\"]     loss = preds.sum()     ...     return loss</p> <pre><code>\n### 4) Single-pass loss aggregation on-device\n\n- What: Compute `loss = preds.sum()` directly from the final trajectory slice.\n- Why it helps: Avoids copying intermediate results back to the CPU and re-iterating in Python.\n- How it helps: Keeps reductions in optimized tensor ops.\n\n```84:88:example_v2.py\n    traj = runner.state_trajectory[-1][-1]\n    preds = traj[\"environment\"][\"daily_infected\"]\n    loss = preds.sum()\n</code></pre>"},{"location":"tutorials/example_v2_optimizations/#5-targeted-parameter-updates-no-per-parameter-python-loops","title":"5) Targeted parameter updates (no per-parameter Python loops)","text":"<ul> <li>What: Build a small <code>params_dict</code> and call <code>runner._set_parameters</code> once.</li> <li>Why it helps: Fewer Python attribute lookups and dynamic dispatches.</li> <li>How it helps: Consolidates mutation into a single call; avoids repeated crossing of Python\u2194backend boundaries.</li> </ul> <p>```109:113:example_v2.py     input_string = \"initializer.transition_function.0.new_transmission.learnable_args.R2\"     params_dict = {input_string: new_tensor}     runner._set_parameters(params_dict)</p> <pre><code>\n### 6) Coarse-grained timing with `perf_counter`\n\n- What: Time loader/executor construction, `runner.init()`, and simulation separately.\n- Why it helps: Identifies the real bottlenecks (I/O vs. initialization vs. step kernel time) quickly.\n- How it helps: Guides where to invest optimization effort (e.g., caching data vs. kernel fusion).\n\n```52:76:example_v2.py\ndef setup(model, population):\n    t0 = time.perf_counter()\n    t_loader_start = time.perf_counter()\n    pop_loader = LoadPopulation(population)\n    dl = DataLoader(model, pop_loader)\n    simulation = Executor(model=model, data_loader=dl)\n    t_loader_end = time.perf_counter()\n    runner = simulation.runner\n    t_init_start = time.perf_counter()\n    runner.init()\n    t_init_end = time.perf_counter()\n    ...\n    return runner\n</code></pre>"},{"location":"tutorials/example_v2_optimizations/#7-optional-kernelperf-telemetry","title":"7) Optional kernel/perf telemetry","text":"<ul> <li>What: Query <code>runner.get_performance_stats()</code> if available.</li> <li>Why it helps: Surfaces GPU/CPU timing breakdowns from the runtime without adding profiling overhead everywhere.</li> <li>How it helps: Quick feedback loop for tuning step sizes, batch shapes, or device placement.</li> </ul> <p>```89:96:example_v2.py     if hasattr(runner, 'get_performance_stats'):         stats = runner.get_performance_stats()         print(\"\\nPerformance Stats:\")         for key, value in stats.items():             print(f\"   {key}: {value}\")</p> <pre><code>\n### 8) DataLoader/LoadPopulation pre-processing (I/O efficiency)\n\n- What: Use `LoadPopulation` + `DataLoader` to encapsulate population I/O and preprocessing.\n- Why it helps: Centralizes data loading, caching, and potential format conversions away from the hot loop.\n- How it helps: Reduces per-run I/O and keeps the simulation loop compute-bound.\n\n```56:61:example_v2.py\n    pop_loader = LoadPopulation(population)\n    dl = DataLoader(model, pop_loader)\n    simulation = Executor(model=model, data_loader=dl)\n</code></pre> <p>In combination, these choices keep compute on the right device, reduce Python overhead, avoid unnecessary transfers, and surface timing where it matters\u2014yielding faster, more predictable runs.</p>"},{"location":"tutorials/example_v2_optimizations/#enginelevel-optimizations-runnerexecutorinitializer","title":"Engine\u2011level optimizations (Runner/Executor/Initializer)","text":"<p>Beyond the simple loop patterns above, the runtime includes several built\u2011in optimizations that improve scaling and latency:</p>"},{"location":"tutorials/example_v2_optimizations/#runner-cuda-path","title":"Runner (CUDA path)","text":"<ul> <li>GPU streams and async transfers: overlaps minimal snapshot I/O with compute, reducing stalls.</li> <li>Memory pooling and tensor reuse: cuts allocator churn and fragmentation; fewer device alloc/free cycles.</li> <li>Vectorized processing hooks: processes many agents in batched ops rather than Python loops.</li> <li>Active\u2011set batching: updates only relevant agents (e.g., exposed/infected), keeping per\u2011step cost proportional to active agents.</li> <li>Minimal/env\u2011only snapshots: bounds trajectory payload; optional downcasting (e.g., int64\u2192int32, bool\u2192uint8).</li> <li>Optional mixed precision: leverages fp16/bf16 where profitable to increase throughput.</li> </ul> <p>Why it helps: less time in the Python interpreter and allocator, fewer/lighter device transfers, and more FLOPs devoted to the subset of agents that actually changed.</p>"},{"location":"tutorials/example_v2_optimizations/#vectorizedrunner","title":"VectorizedRunner","text":"<ul> <li>Detects and uses vectorized implementations (e.g., via vmap) when available, falling back otherwise.</li> <li>Reduces host overhead and enables fused batched kernels for observation/policy/transition.</li> </ul> <p>Why it helps: shifts work from many small ops to fewer larger ops, improving arithmetic intensity and kernel efficiency.</p>"},{"location":"tutorials/example_v2_optimizations/#initializer-and-data-residency","title":"Initializer and data residency","text":"<ul> <li>Single device move at init: tensor leaves are moved once; subsequent steps keep data resident on GPU.</li> <li>Dedicated snapshot stream: subsequent snapshots are async and minimal rather than full copies.</li> </ul> <p>Why it helps: avoids repeated CPU\u2194GPU shuttling and keeps the hot path compute\u2011bound.</p>"},{"location":"tutorials/example_v2_optimizations/#utils-and-graph-representation-gpu-path","title":"Utils and graph representation (GPU path)","text":"<ul> <li>Sparse edge representations instead of dense adjacency matrices in contact networks.</li> </ul> <p>Why it helps: lowers memory footprint and avoids O(N^2) dense ops, improving performance on large populations.</p>"},{"location":"tutorials/example_v2_optimizations/#perf-scaffolding","title":"Perf scaffolding","text":"<ul> <li>Built\u2011in counters/timers (allocations, reuses, vectorized ops, transfer counts) and <code>get_performance_stats()</code>.</li> </ul> <p>Why it helps: fast feedback to tune batch sizes, active\u2011set thresholds, and snapshot frequency without full profilers.</p>"},{"location":"tutorials/example_v2_optimizations/#evidence-of-impact","title":"Evidence of impact","text":"<ul> <li>See <code>analyze_performance/perf-astoria-cuda-vs-cpu.md</code>: CUDA + optimized utils achieve ~71.6\u00d7 total speedup vs CPU base for Astoria (37,518 agents), with step time ~138\u00d7 faster. Scaling to NYC shows good efficiency with much larger populations.</li> </ul>"},{"location":"tutorials/calibrating-a-model/","title":"Calibrating an AgentTorch Model","text":"<p>This tutorial demonstrates how to calibrate parameters in an AgentTorch model using different optimization approaches. We'll explore three methods for parameter optimization and discuss when to use each approach.</p>"},{"location":"tutorials/calibrating-a-model/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of PyTorch and gradient-based optimization</li> <li>Familiarity with AgentTorch's basic concepts</li> <li>Python 3.8+</li> <li>Required packages listed in <code>requirements.txt</code></li> </ul>"},{"location":"tutorials/calibrating-a-model/#installation","title":"Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#overview","title":"Overview","text":"<p>Model calibration is a crucial step in agent-based modeling. AgentTorch provides several approaches to optimize model parameters:</p> <ol> <li>Internal parameter optimization</li> <li>External parameter optimization</li> <li>Generator-based parameter optimization</li> </ol>"},{"location":"tutorials/calibrating-a-model/#basic-setup","title":"Basic Setup","text":"<p>First, let's set up our environment and import the necessary modules:</p> <pre><code>import warnings\nwarnings.simplefilter(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom agent_torch.models import covid\nfrom agent_torch.populations import sample\nfrom agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\n\n# Initialize simulation\nsim = Executor(covid, pop_loader=LoadPopulation(sample))\nrunner = sim.runner\nrunner.init()\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#helper-classes-and-functions","title":"Helper Classes and Functions","text":"<p>We'll define some helper components that we'll use throughout the tutorial:</p> <pre><code>class LearnableParams(nn.Module):\n    \"\"\"A neural network module that generates bounded parameters\"\"\"\n    def __init__(self, num_params, device='cpu'):\n        super().__init__()\n        self.device = device\n        self.num_params = num_params\n        self.learnable_params = nn.Parameter(torch.rand(num_params, device=self.device))\n        self.min_values = torch.tensor(2.0, device=self.device)\n        self.max_values = torch.tensor(3.5, device=self.device)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self):\n        out = self.learnable_params\n        # Bound output between min_values and max_values\n        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n        return out\n\ndef execute(runner, n_steps=5):\n    \"\"\"Execute simulation and compute loss\"\"\"\n    runner.step(n_steps)\n    labels = runner.state_trajectory[-1][-1]['environment']['daily_infected']\n    return labels.sum()\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#method-1-internal-parameter-optimization","title":"Method 1: Internal Parameter Optimization","text":"<p>The first approach optimizes parameters that are internal to the simulation. This is useful when you want to directly optimize parameters that are part of your model's structure.</p> <pre><code>def optimize_internal_params():\n    # Execute simulation and compute gradients\n    loss = execute(runner)\n    loss.backward()\n\n    # Get gradients of learnable parameters\n    learn_params_grad = [(name, param, param.grad) \n                        for (name, param) in runner.named_parameters()]\n    return learn_params_grad\n\n# Example usage\ngradients = optimize_internal_params()\nprint(\"Internal parameter gradients:\", gradients)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#when-to-use-this-method","title":"When to use this method?","text":"<ul> <li>When parameters are naturally part of your simulation structure</li> <li>When you want direct control over parameter optimization</li> <li>For simpler models with fewer parameters</li> </ul>"},{"location":"tutorials/calibrating-a-model/#method-2-external-parameter-optimization","title":"Method 2: External Parameter Optimization","text":"<p>The second approach involves optimizing external parameters that are fed into the simulation. This provides more flexibility in parameter management.</p> <pre><code>def optimize_external_params():\n    # Create external parameters\n    external_params = nn.Parameter(\n        torch.tensor([2.7, 3.8, 4.6], requires_grad=True)[:, None]\n    )\n\n    # Set parameters in the runner\n    learnable_params = runner.named_parameters()\n    params_dict = {next(iter(learnable_params))[0]: external_params}\n    runner._set_parameters(params_dict)\n\n    # Execute and compute gradients\n    loss = execute(runner)\n    loss.backward()\n    return external_params.grad\n\n# Example usage\ngradients = optimize_external_params()\nprint(\"External parameter gradients:\", gradients)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#when-to-use-this-method_1","title":"When to use this method?","text":"<ul> <li>When you want to manage parameters outside the simulation</li> <li>For parameter sweeps or sensitivity analysis</li> <li>When parameters need to be shared across different components</li> </ul>"},{"location":"tutorials/calibrating-a-model/#method-3-generator-based-parameter-optimization","title":"Method 3: Generator-Based Parameter Optimization","text":"<p>The third approach uses a generator function to predict optimal parameters. This is particularly useful for complex parameter relationships.</p> <pre><code>def optimize_with_generator():\n    # Create generator model\n    learn_model = LearnableParams(3)\n    params = learn_model()[:, None]\n\n    # Execute and compute gradients\n    loss = execute(runner)\n    loss.backward()\n\n    # Get gradients of generator parameters\n    learn_params_grad = [(param, param.grad) \n                        for (name, param) in learn_model.named_parameters()]\n    return learn_params_grad\n\n# Example usage\ngradients = optimize_with_generator()\nprint(\"Generator parameter gradients:\", gradients)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#when-to-use-this-method_2","title":"When to use this method?","text":"<ul> <li>When parameters have complex relationships</li> <li>For learning parameter patterns</li> <li>When you want to generate parameters based on conditions</li> </ul>"},{"location":"tutorials/calibrating-a-model/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's how to use all three methods in a complete optimization loop:</p> <pre><code>def calibrate_model(method='internal', num_epochs=10):\n    for epoch in range(num_epochs):\n        if method == 'internal':\n            gradients = optimize_internal_params()\n        elif method == 'external':\n            gradients = optimize_external_params()\n        else:  # generator\n            gradients = optimize_with_generator()\n\n        print(f\"Epoch {epoch}, gradients: {gradients}\")\n        # Add your optimizer step here\n\n# Example usage\ncalibrate_model(method='internal', num_epochs=3)\n</code></pre>"},{"location":"tutorials/calibrating-a-model/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Method: Consider your specific use case when selecting an optimization approach.</li> <li>Monitor Convergence: Always track your loss function to ensure proper optimization.</li> <li>Validate Results: Cross-validate your calibrated parameters with held-out data.</li> <li>Handle Constraints: Use appropriate bounds and constraints for your parameters.</li> </ol>"},{"location":"tutorials/calibrating-a-model/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>Ensure parameters have appropriate ranges</li> <li>Watch out for local optima</li> <li>Be careful with learning rates in optimization</li> <li>Consider the computational cost of each approach</li> </ul>"},{"location":"tutorials/calibrating-a-model/#conclusion","title":"Conclusion","text":"<p>We've explored three different approaches to model calibration in AgentTorch. Each method has its strengths and is suited for different scenarios. Choose the approach that best matches your specific needs and model complexity.</p>"},{"location":"tutorials/calibrating-a-model/#additional-resources","title":"Additional Resources","text":"<ul> <li>AgentTorch Documentation</li> <li>PyTorch Optimization</li> <li>Related Tutorials</li> </ul>"},{"location":"tutorials/compare-llm-performance/","title":"Benchmark: Sensitivity to Agent Behaviors","text":""},{"location":"tutorials/compare-llm-performance/#exploring-llm-influence","title":"Exploring LLM Influence","text":""},{"location":"tutorials/compare-llm-performance/#introduction","title":"Introduction","text":"<p>AgentTorch is a framework that scales ABM simulations to real-world problems. This tutorial will guide you through the process of experimenting with different LLMs in AgentTorch to understand their impact on the framework's effectiveness.</p>"},{"location":"tutorials/compare-llm-performance/#step-1-setup","title":"Step 1: Setup","text":"<p>First, let's set up our environment and import the necessary libraries:</p> <pre><code>import sys\nfrom dspy_modules import COT, BasicQAWillToWork\nfrom agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.populations import NYC\nfrom agent_torch.core.llm.backend import DspyLLM, LangchainLLM\n\nimport torch\nOPENAI_API_KEY = None\n</code></pre> <p>Setup : Covid Cases Data and Unemployment Rate</p> <pre><code>from utils import get_covid_cases_data\ncsv_path = 'agent_torch/models/covid/data/county_data.csv'\nmonthly_cases_kings = get_covid_cases_data(csv_path=csv_path,county_name='Kings County')\nmonthly_cases_queens = get_covid_cases_data(csv_path=csv_path,county_name='Queens County')\nmonthly_cases_bronx = get_covid_cases_data(csv_path=csv_path,county_name='Bronx County')\nmonthly_cases_new_york = get_covid_cases_data(csv_path=csv_path,county_name='New York County')\nmonthly_cases_richmond = get_covid_cases_data(csv_path=csv_path,county_name='Richmond County')\n\n</code></pre>"},{"location":"tutorials/compare-llm-performance/#step-2-initialise-llm-instance","title":"Step 2: Initialise LLM Instance","text":"<p>We can use either of the Langchain and Dspy backends to initialise a LLM instance. While these are the frameworks we are supporting currently, you may choose to use your own framework of choice by extending the LLMBackend class provided with AgentTorch.</p> <p>Let's see how we can use Langchain to initialise an LLM instance</p> <p>GPT 3.5 Turbo</p> <pre><code>agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\nllm_langchain_35 = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\"\n)\n</code></pre> <p>GPT 4-0 Mini</p> <pre><code>agent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\nllm_langchain_4o_mini = LangchainLLM(\n    openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-4o-mini\"\n) \n</code></pre> <p>Similarly if we wanted to use Dspy backend, we can instantiate the DspyLLM object. We can pass the desired model name as argument just like we did with Langchain.</p> <pre><code># Agent profile is decided by the QA module and the COT module enforces Chain of Thought reasoning\nllm_dspy = DspyLLM(qa=BasicQAWillToWork, cot=COT, openai_api_key=OPENAI_API_KEY)\n</code></pre>"},{"location":"tutorials/compare-llm-performance/#step-3-define-agent-behavior","title":"Step 3: Define agent Behavior","text":"<p>Create an object of the Behavior class You have options to pass any of the above created llm objects to the behavior class Specify the region for which the behavior is to be generated. This should be the name of any of the regions available in the populations folder.</p> <pre><code># Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=2) \n\n# Define a prompt template\n# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\nuser_prompt_template = \"Your age is {age}, gender is {gender}, ethnicity is {ethnicity}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n\n# Create a behavior model\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\nearning_behavior_4o_mini = Behavior(\n    archetype=archetype.llm(llm=llm_langchain_4o_mini, user_prompt=user_prompt_template),\n    region=NYC\n)\nearning_behavior_35 = Behavior(\n    archetype=archetype.llm(llm=llm_langchain_35, user_prompt=user_prompt_template),\n    region=NYC\n)\n</code></pre> <pre><code># Define arguments to be used for creating a query for the LLM instance\nkwargs = {\n    \"month\": \"January\",\n    \"year\": \"2020\",\n    \"covid_cases\": 1200,\n    \"device\": \"cpu\",\n    \"current_memory_dir\": \"/populations/astoria/conversation_history\",\n    \"unemployment_rate\": 0.05,\n}\n</code></pre>"},{"location":"tutorials/compare-llm-performance/#step-4-compare-performance-between-different-llm-models","title":"Step 4: Compare performance between different LLM models","text":"<pre><code>from utils import get_labor_data, get_labor_force_correlation\n\nlabor_force_df_4o_mini, observed_labor_force_4o_mini, correlation_4o_mini = get_labor_force_correlation(\n    monthly_cases_kings, \n    earning_behavior_4o_mini, \n    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n    kwargs\n)\nlabor_force_df_35, observed_labor_force_35, correlation_35 = get_labor_force_correlation(\n    monthly_cases_kings, \n    earning_behavior_35, \n    'agent_torch/models/macro_economics/data/unemployment_rate_csvs/Brooklyn-Table.csv',\n    kwargs\n)\nprint(f\"Correlation with GPT 3.5 is {correlation_35} and with GPT 4o Mini is {correlation_4o_mini}\")\n</code></pre>"},{"location":"tutorials/config_api/","title":"AgentTorch Configuration API Tutorial","text":"<p>This tutorial will guide you through using AgentTorch's Configuration API to set up simulations. We'll use a simple movement simulation as our primary example and then explore advanced features.</p>"},{"location":"tutorials/config_api/#basic-usage-movement-simulation","title":"Basic Usage: Movement Simulation","text":"<p>Let's create a simulation where agents move randomly within a bounded space. The full code is available here. This example demonstrates the core concepts of AgentTorch's configuration system.</p>"},{"location":"tutorials/config_api/#project-structure","title":"Project Structure","text":"<pre><code>agent_torch/examples/models/movement/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 substeps/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 random_move.py\n\u2502   \u2514\u2500\u2500 update_position.py\n\u2514\u2500\u2500 yamls/\n    \u2514\u2500\u2500 config.yaml\n</code></pre>"},{"location":"tutorials/config_api/#setting-up-the-configuration","title":"Setting Up the Configuration","text":"<p>First, import the necessary builders:</p> <pre><code>from agent_torch.config import (\n    ConfigBuilder,\n    StateBuilder,\n    AgentBuilder,\n    PropertyBuilder,\n    EnvironmentBuilder,\n    SubstepBuilder,\n    PolicyBuilder,\n    TransitionBuilder\n)\n</code></pre>"},{"location":"tutorials/config_api/#1-simulation-metadata","title":"1. Simulation Metadata","text":"<p>Define the basic simulation parameters (note that all these are REQUIRED PARAMETERS):</p> <pre><code>metadata = {\n    \"num_agents\": 1000,\n    \"num_episodes\": 10,\n    \"num_steps_per_episode\": 20,\n    \"num_substeps_per_step\": 1,\n    \"device\": \"cpu\",\n    \"calibration\": False # if learnable parameters will be optimized externally [more details in gradient-based calibration tutorial]\n}\nconfig.set_metadata(metadata)\n</code></pre>"},{"location":"tutorials/config_api/#2-state-configuration","title":"2. State Configuration","text":"<p>Configure the simulation state including agents and environment:</p> <pre><code># Build state\nstate_builder = StateBuilder()\n\n# Add agent with position property\nagent_builder = AgentBuilder(\"citizens\", metadata[\"num_agents\"])\nposition = PropertyBuilder(\"position\")\\\n    .set_dtype(\"float\")\\\n    .set_shape([metadata[\"num_agents\"], 2])\\\n    .set_value([0.0, 0.0])\nagent_builder.add_property(position)\nstate_builder.add_agent(\"citizens\", agent_builder)\n\n# Add environment bounds\nenv_builder = EnvironmentBuilder()\nbounds = PropertyBuilder(\"bounds\")\\\n    .set_dtype(\"float\")\\\n    .set_shape([2])\\\n    .set_value([100.0, 100.0])\nenv_builder.add_variable(bounds)\nstate_builder.set_environment(env_builder)\n\n# Set state in config\nconfig.set_state(state_builder.to_dict())\n</code></pre>"},{"location":"tutorials/config_api/#3-substep-configuration","title":"3. Substep Configuration","text":"<p>Define the simulation substeps with their policies and transitions:</p> <pre><code># Create movement substep\nmovement = SubstepBuilder(\"Movement\", \"Agent movement simulation\")\nmovement.add_active_agent(\"citizens\")\nmovement.config[\"observation\"] = {\"citizens\": None}\n\n# Add movement policy\npolicy = PolicyBuilder()\nstep_size = PropertyBuilder.create_argument(\n    name=\"Step size parameter\",\n    value=1.0,\n    learnable=True\n).config\n\npolicy.add_policy(\n    \"move\",\n    \"RandomMove\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"direction\"],\n    {\"step_size\": step_size}\n)\nmovement.set_policy(\"citizens\", policy)\n\n# Add movement transition\ntransition = TransitionBuilder()\nbounds_param = PropertyBuilder.create_argument(\n    name=\"Environment bounds\",\n    value=[100.0, 100.0],\n    shape=[2],\n    learnable=True\n).config\n\ntransition.add_transition(\n    \"update_position\",\n    \"UpdatePosition\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"position\"],\n    {\"bounds\": bounds_param}\n)\nmovement.set_transition(transition)\n\n# Add substep to config and save\nconfig.add_substep(\"0\", movement)\nconfig.save_yaml(\"models/movement/yamls/config.yaml\")\n</code></pre>"},{"location":"tutorials/config_api/#4-manual-substep-implementation","title":"4. Manual Substep Implementation","text":"<p>The substeps can be implemented manually as Python classes. Here's an example of the movement policy:</p> <pre><code>@Registry.register_substep(\"move\", \"policy\")\nclass RandomMove(SubstepAction):\n    def forward(self, state: Dict[str, Any], observations) -&gt; Dict[str, Any]:\n        positions = get_var(state, self.input_variables[\"position\"])\n        num_agents = positions.shape[0]\n\n        # Get step size from learnable arguments\n        self.step_size = self.learnable_args[\"step_size\"]\n\n        # Generate random angles and directions\n        angles = torch.rand(num_agents) * 2 * torch.pi\n        direction = torch.stack([\n            torch.cos(angles) * self.step_size,\n            torch.sin(angles) * self.step_size\n        ], dim=1)\n\n        # Return using output variable name from config\n        outputs = {}\n        outputs[self.output_variables[0]] = direction\n        return outputs\n</code></pre>"},{"location":"tutorials/config_api/#5-setting-up-module-structure","title":"5. Setting Up Module Structure","text":"<p>The substeps need to be properly imported and registered. This is done through <code>__init__.py</code> files:</p> <pre><code># models/movement/substeps/__init__.py\n\"\"\"Substep implementations for movement simulation.\"\"\"\nfrom .random_move import *\nfrom .update_position import *\n</code></pre> <pre><code># models/movement/__init__.py\n\"\"\"Movement simulation model.\"\"\"\nfrom agent_torch.core import Registry\nfrom agent_torch.core.helpers import *\n\nfrom .substeps import *\n\n# Create and populate registry as a module-level variable\nregistry = Registry()\n</code></pre> <p>These files ensure that: 1. All substep implementations are imported when the movement module is imported 2. The registry is created at the module level 3. Substeps are automatically registered via their decorators when imported</p>"},{"location":"tutorials/config_api/#6-running-the-simulation","title":"6. Running the Simulation","text":"<p>The movement simulation can be run using:</p> <pre><code># run_movement_sim.py\nfrom agent_torch.populations import sample2\nfrom agent_torch.examples.models import movement\nfrom agent_torch.core.environment import envs\n\ndef run_movement_simulation():\n    \"\"\"Run the movement simulation.\"\"\"\n    # Create simulation runner\n    runner = envs.create(\n        model=movement,\n        population=sample2\n    )\n\n    # Get simulation parameters\n    sim_steps = runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"]\n    num_episodes = runner.config[\"simulation_metadata\"][\"num_episodes\"]\n\n    # Run episodes\n    for episode in range(num_episodes):\n        if episode &gt; 0:\n            runner.reset()\n        runner.step(sim_steps)\n\n        # Print statistics\n        positions = runner.state[\"agents\"][\"citizens\"][\"position\"]\n        print(f\"Episode {episode + 1} - Average position: {positions.mean(dim=0)}\")\n\nif __name__ == \"__main__\":\n    runner = run_movement_simulation()\n</code></pre> <p>To run the simulation:</p> <pre><code>python -m agent_torch.examples.run_movement_sim\n</code></pre>"},{"location":"tutorials/config_api/#advanced-usage-automatic-substep-generation","title":"Advanced Usage: Automatic Substep Generation","text":"<p>Instead of writing substeps manually, AgentTorch provides <code>SubstepBuilderWithImpl</code> to automatically generate implementation templates. Let's see how we could have used it for our movement example:</p> <pre><code>from agent_torch.config import SubstepBuilderWithImpl\n\n# Create substep with implementation generation\nmovement_substep = SubstepBuilderWithImpl(\n    name=\"Movement\", \n    description=\"Agent movement simulation\",\n    output_dir=\"models/movement/substeps\"\n)\nmovement_substep.add_active_agent(\"citizens\")\nmovement_substep.config[\"observation\"] = {\"citizens\": None}\n\n# Add same policy and transition configurations\npolicy = PolicyBuilder()\npolicy.add_policy(\n    \"move\",\n    \"RandomMove\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"direction\"],\n    {\"step_size\": step_size}\n)\nmovement_substep.add_policy(\"citizens\", policy)\n\ntransition = TransitionBuilder()\ntransition.add_transition(\n    \"update_position\",\n    \"UpdatePosition\",\n    {\"position\": \"agents/citizens/position\"},\n    [\"position\"],\n    {\"bounds\": bounds_param}\n)\nmovement_substep.set_transition(transition)\n\n# Generate implementation files\ngenerated_files = movement_substep.generate_implementations()\n</code></pre> <p>This will generate template files for both <code>random_move.py</code> and <code>update_position.py</code> with: - Proper imports and class structure - Registry decorators - Input/output variable handling - TODO sections for implementation logic</p> <p>You would then only need to fill in the forward logic in the TODO sections.</p>"},{"location":"tutorials/config_api/#more-examples","title":"More Examples","text":"<p>For a more complex example, check out the COVID-19 simulation in <code>agent_torch/examples/models/covid/</code>. This example demonstrates: - Multiple substeps with complex interactions - Custom observation and reward functions - Network-based agent interactions - Advanced use of learnable parameters</p> <p>You can run it with:</p> <pre><code>python -m agent_torch.examples.run_covid_sim\n</code></pre>"},{"location":"tutorials/config_api/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Metadata: Always include required fields like <code>num_agents</code>, <code>num_episodes</code>, <code>num_steps_per_episode</code>, and <code>num_substeps_per_step</code>.</p> </li> <li> <p>PropertyBuilder Arguments: Use <code>PropertyBuilder.create_argument()</code> for learnable parameters, and access them in substeps via <code>self.learnable_args</code> or <code>self.arguments</code> (if not learnable).</p> </li> <li> <p>Output Variables: Always use <code>self.output_variables[index]</code> as dictionary keys when returning from substep forward methods to ensure consistency with the configuration.</p> </li> <li> <p>Observation Structure: When no observation is needed, use <code>{\"citizens\": None}</code> instead of an empty dict.</p> </li> <li> <p>Implementation Generation: Use <code>SubstepBuilderWithImpl</code> for complex simulations to automatically generate consistent substep templates. This is especially helpful when you have many substeps or want to ensure consistent structure across your codebase. </p> </li> </ol>"},{"location":"tutorials/configure-behavior/","title":"Guide to Prompting LLM as ABM Agents","text":"<p>Welcome to this comprehensive tutorial on AI agent behavior generation! This guide is designed for newcomers who want to learn how to create AI agents and simulate population behaviors using a custom framework. We'll walk you through each step, explaining concepts as we go.</p>"},{"location":"tutorials/configure-behavior/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction to the Framework</li> <li>Setting Up Your Environment</li> <li>Understanding the Core Components</li> <li>Creating Your First AI Agent</li> <li>Generating Population Behaviors</li> <li>Putting It All Together</li> <li>Next Steps and Advanced Topics</li> </ol>"},{"location":"tutorials/configure-behavior/#1-introduction-to-the-framework","title":"1. Introduction to the Framework","text":"<p>Our framework is designed to simulate population behaviors using AI agents. It combines several key components:</p> <ul> <li>LLM Agents: We use Large Language Models (LLMs) to create intelligent   agents that can make decisions based on given scenarios.</li> <li>Archetypes: These represent different types of individuals in a   population.</li> <li>Behaviors: These simulate how individuals might act in various situations.</li> </ul> <p>This framework is particularly useful for modeling complex social or economic scenarios, such as population responses during a pandemic.</p> <p></p>"},{"location":"tutorials/configure-behavior/#2-setting-up-your-environment","title":"2. Setting Up Your Environment","text":"<p>Now, let's set up your OpenAI API key (you'll need an OpenAI account):</p> <pre><code>OPENAI_API_KEY = None # Replace with your actual API key\n</code></pre> <p></p>"},{"location":"tutorials/configure-behavior/#3-understanding-the-core-components","title":"3. Understanding the Core Components","text":"<p>Let's break down the main components of our framework:</p>"},{"location":"tutorials/configure-behavior/#dspyllm-and-langchainllm","title":"DspyLLM and LangchainLLM","text":"<p>These are wrappers around language models that allow us to create AI agents. They can process prompts and generate responses based on given scenarios.</p>"},{"location":"tutorials/configure-behavior/#archetype","title":"Archetype","text":"<p>This component helps create different \"types\" of individuals in our simulated population. Like Male under 19, Female from 20 to 29 years of age.</p>"},{"location":"tutorials/configure-behavior/#behavior","title":"Behavior","text":"<p>The Behavior component simulates how individuals (or groups) might act in various situations. It uses the AI agents to generate these behaviors.</p> <p>Now you have two AI agents ready to process prompts!</p>"},{"location":"tutorials/configure-behavior/#4-creating-llm-agents","title":"4. Creating LLM Agents","text":"<p>We support using Langchain and Dspy backends to initialize LLM instances - for agent and archetypes. Using our LLMBackend class, you can integrate any framework of your choice.</p>"},{"location":"tutorials/configure-behavior/#using-dspy","title":"Using DSPy","text":"<pre><code>from dspy_modules import COT, BasicQAWillToWork\nfrom agent_torch.core.llm.llm import DspyLLM\n\nllm_dspy = DspyLLM(qa=BasicQAWillToWork, cot=COT, openai_api_key=OPENAI_API_KEY)\nllm_dspy.initialize_llm()\n\noutput_dspy = llm_dspy.prompt([\"You are an individual living during the COVID-19 pandemic. You need to decide your willingness to work each month and portion of your assets you are willing to spend to meet your consumption demands, based on the current situation of NYC.\"])\nprint(\"DSPy Output:\", output_dspy)\n</code></pre>"},{"location":"tutorials/configure-behavior/#using-langchain","title":"Using Langchain","text":"<pre><code>from agent_torch.core.llm.llm import LangchainLLM\n\nagent_profile = \"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0 and 1, only.\"\n\nllm_langchian = LangchainLLM(openai_api_key=OPENAI_API_KEY, agent_profile=agent_profile, model=\"gpt-3.5-turbo\")\nllm_langchian.initialize_llm()\n\noutput_langchain = llm_langchian.prompt([\"You are an helpful agent who is trying to help the user make a decision. Give answer as a single number between 0.0 and 1.0, only.\"])\nprint(\"Langchain Output:\", output_langchain)\n</code></pre>"},{"location":"tutorials/configure-behavior/#5-generating-population-behaviors","title":"5. Generating Population Behaviors","text":"<p>To simulate population behaviors, we'll use the Archetype and Behavior classes:</p> <pre><code>from agent_torch.core.llm.archetype import Archetype\nfrom agent_torch.core.llm.behavior import Behavior\nfrom agent_torch.populations import NYC\n\n# Create an object of the Archetype class\n# n_arch is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.\narchetype = Archetype(n_arch=7)\n\n# Define a prompt template\n# Age,Gender and other attributes which are part of the population data, will be replaced by the actual values of specified region, during the simulation.\n# Other variables like Unemployment Rate and COVID cases should be passed as kwargs to the behavior model.\nuser_prompt_template = \"Your age is {age} {gender}, unemployment rate is {unemployment_rate}, and the number of COVID cases is {covid_cases}.Current month is {month} and year is {year}.\"\n\n# Create a behavior model\n# You have options to pass any of the above created llm objects to the behavior class\n# Specify the region for which the behavior is to be sampled. This should be the name of any of the regions available in the populations folder.\nearning_behavior = Behavior(\n    archetype=archetype.llm(llm=llm_dspy, user_prompt=user_prompt_template, num_agents=12),\n    region=NYC\n)\n\nprint(\"Behavior model created successfully!\")\n</code></pre> <p>This sets up a behavior model that can simulate how 12 different agents might behave in NYC during the COVID-19 pandemic.</p> <p></p>"},{"location":"tutorials/configure-behavior/#6-putting-it-all-together","title":"6. Putting It All Together","text":"<p>Now, let's use our behavior model to generate some population behaviors:</p> <pre><code># Define scenario parameters\n# Names of the parameters should match the placeholders in the user_prompt template\nscenario_params = {\n    'month': 'January',\n    'year': '2020',\n    'covid_cases': 1200,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.05,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <pre><code># Define another scenario parameters\nscenario_params = {\n    'month': 'February',\n    'year': '2020',\n    'covid_cases': 900,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.1,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <pre><code># Define yet another scenario parameters\nscenario_params = {\n    'month': 'March',\n    'year': '2020',\n    'covid_cases': 200,\n    'device': 'cpu',\n    'current_memory_dir': '/path-to-save-memory',\n    'unemployment_rate': 0.11,\n}\n\n# Generate behaviors\npopulation_behaviors = earning_behavior.sample(scenario_params)\nprint(\"Population Behaviors:\")\nprint(population_behaviors)\n</code></pre> <p>And so on...</p> <p>This will output a set of behaviors for our simulated population based on the given scenario.</p> <p></p>"},{"location":"tutorials/configure-behavior/#7-next-steps-and-advanced-topics","title":"7. Next Steps and Advanced Topics","text":"<p>You've just created your first AI agents and simulated population behaviors. Here are some advanced topics you might want to explore next:</p> <ul> <li>Customizing archetypes for specific populations</li> <li>Creating more complex behavior models</li> </ul>"},{"location":"tutorials/creating-a-model/","title":"Predator-Prey Model","text":"Imports <pre><code># import agent-torch\n\nimport os\nimport sys\nmodule_path = os.path.abspath(os.path.join('../../../agent_torch'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\n\nfrom AgentTorch import Runner, Registry\nfrom AgentTorch.substep import SubstepObservation, SubstepAction, SubstepTransition\nfrom AgentTorch.helpers import get_by_path, read_config, read_from_file, grid_network\n</code></pre> <pre><code># import all external libraries that we need.\n\nimport math\nimport torch\nimport re\nimport random\nimport argparse\nimport numpy as np\nimport torch.nn as nn\nimport networkx as nx\nimport osmnx as ox\nfrom tqdm import trange\n</code></pre> <pre><code># define the helper functions we need.\n\ndef get_var(state, var):\n  \"\"\"\n    Retrieves a value from the current state of the model.\n  \"\"\"\n  return get_by_path(state, re.split('/', var))\n</code></pre> <p>The complete code for this model can be found here. The architecture of the AgentTorch framework, which explains some key concepts, can be found here.</p> <p>This guide walks you through creating a custom predator-prey model using the AgentTorch framework. This model will simulate an ecosystem consisting of predators, prey and grass: predators eat prey, and prey eat grass.</p> <p>The model's parameters, rules and configuration are passed to AgentTorch, which iteratively simulates the model, allowing you to optimize its learnable parameters, while also modelling the simulation in real time. AgentTorch's Python API is based on PyTorch, which enhances its performance on GPUs.</p> <p>The following sections detail:</p> <ul> <li>an overview of the model's rules and parameters.</li> <li>the properties of all entities stored in the model's state.</li> <li>the substeps that observe, simulate and modify the state for each agent.</li> <li>the code required to run the simulation using <code>agent-torch</code>.</li> <li>plotting the state's trajectory using <code>matplotlib</code>.</li> </ul>"},{"location":"tutorials/creating-a-model/#model-overview","title":"Model Overview","text":"<p>The following are configurable parameters of the model:</p> <ul> <li>a $n \\times m$ grid, with $p$ predators and $q$ prey to start with.</li> <li>grass can grown on any of the $n \\cdot m$ squares in the grid.</li> </ul> <p>The rules followed by the simulated interactions are configured as follows:</p> <ul> <li>predators can eat only prey, and prey can eat only grass.</li> <li>grass grows back once eaten after a certain number of steps.</li> <li>upon consuming food, the energy of the consumer increases.</li> <li>movement happens randomly, to any neighbouring square in the grid.</li> <li>each move reduces the energy of the entity by a fixed amount.</li> </ul> <p>These parameters and rules, along with the properties of the entities (detailed below) in the simulation are defined in a configuration file, and passed on to the model.</p>"},{"location":"tutorials/creating-a-model/#state-environment-agents-and-objects","title":"State: Environment, Agents, and Objects","text":"<p>The model's state consists of a list of properties of the simulated environment, and the agents and objects situated in that simulation. For this model, the:</p>"},{"location":"tutorials/creating-a-model/#environment","title":"Environment","text":"<p>The environment will have only one property: the size of the two-dimensional grid in which the predators and prey wander, defined like so:</p> <pre><code>environment:\n  bounds: (max_x, max_y) # tuple of integers\n</code></pre>"},{"location":"tutorials/creating-a-model/#agents","title":"Agents","text":"<p>This model has two agents: predator, and prey.</p>"},{"location":"tutorials/creating-a-model/#predator","title":"Predator","text":"<p>The predator agent is defined like so:</p> <pre><code>predator:\n  coordinates: (x, y) # tuple of integers\n  energy: float\n  stride_work: float\n</code></pre> <p>The <code>coordinates</code> property depicts the current position of the predator in the two-dimensional grid. It is initialized from a CSV file that contains a list of randomly generated coordinates for all 40 predators.</p> <p>The <code>energy</code> property stores the current amount of energy possessed by the predator. Initially, this property is set to a random number between 30 and 100.</p> <p>The <code>stride_work</code> property is a static, but learnable property that stores the amount of energy to deduct from a predator for one step in any direction on the grid.</p>"},{"location":"tutorials/creating-a-model/#prey","title":"Prey","text":"<p>The prey agent is identical to the predator agent, and has one additional property: <code>nutritional_value</code>.</p> <pre><code>prey:\n  coordinates: (x, y) # tuple of integers\n  energy: float\n  stride_work: float\n  nutritional_value: float\n</code></pre> <p>The <code>nutritional_value</code> property is a static but learnable property that stores the amount of energy gained by a predator when it consumes a single prey entity.</p>"},{"location":"tutorials/creating-a-model/#objects","title":"Objects","text":"<p>This model has only one agent: grass.</p>"},{"location":"tutorials/creating-a-model/#grass","title":"Grass","text":"<p>The grass entity is defined as follows:</p> <pre><code>grass:\n  coordinates: (x, y)\n  growth_stage: 0|1\n  growth_countdown: float\n  regrowth_time: float\n  nutritional_value: float\n</code></pre> <p>The <code>coordinates</code> property depicts the current position of the predator in the two-dimensional grid. It is initialized from a CSV file that contains a list of all 1600 coordinates.</p> <p>The <code>growth_stage</code> property stores the current growth stage of the grass: 0 means it is growing, and 1 means it is fully grown.</p> <p>The <code>growth_countdown</code> property stores the number of steps after which the grass becomes fully grown. The <code>regrowth_time</code> property is static and learnable, and stores the max value of the countdown property.</p> <p>The <code>nutritional_value</code> property is a static but learnable property that stores the amount of energy gained by a predator when it consumes a single prey entity.</p>"},{"location":"tutorials/creating-a-model/#network","title":"Network","text":"<p>The model makes use of the adjacency matrix of a two-dimensional grid filled with predator and prey to simulate the movement of those entities.</p> <pre><code>network:\n  agent_agent:\n    grid: [predator, prey]\n</code></pre>"},{"location":"tutorials/creating-a-model/#substeps","title":"Substeps","text":"<p>Each substep is a <code>torch.nn.ModuleDict</code> that takes an input state, and produces an updated state as output. A substep consists of three phases:</p> <ol> <li>Observation (retrieving relevant information from the state)</li> <li>Policy/Action (deciding on the course of action as per the observations)</li> <li>Transition (randomizing and updating the state according to the action)</li> </ol> <p>This model consists of four substeps: <code>move</code>, <code>eat_grass</code>, <code>hunt_prey</code>, and <code>grow_grass</code>.</p> Helper functions <pre><code># define all the helper functions we need.\n\ndef get_neighbors(pos, adj_grid, bounds):\n  \"\"\"\n    Returns a list of neighbours for each position passed in the given\n    `pos` tensor, using the adjacency matrix passed in `adj_grid`.\n  \"\"\"\n  x, y = pos\n  max_x, max_y = bounds\n\n  # calculate the node number from the x, y coordinate.\n  # each item (i, j) in the adjacency matrix, if 1 depicts\n  # that i is connected to j and vice versa.\n  node = (max_y * x) + y\n  conn = adj_grid[node]\n\n  neighbors = []\n  for idx, cell in enumerate(conn):\n    # if connected, calculate the (x, y) coords of the other\n    # node and add it to the list of neighbors.\n    if cell == 1:\n      c = (int) (idx % max_y)\n      r = math.floor((idx - c) / max_y)\n\n      neighbors.append(\n        [torch.tensor(r), torch.tensor(c)]\n      )\n\n  return torch.tensor(neighbors)\n\n# define a function to retrieve the input required\ndef get_find_neighbors_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    adj_grid = get_var(state, input_variables['adj_grid'])\n    positions = get_var(state, input_variables['positions'])\n\n    return bounds, adj_grid, positions\n\ndef get_decide_movement_input(state, input_variables):\n    positions = get_var(state, input_variables['positions'])\n    energy = get_var(state, input_variables['energy'])\n\n    return positions, energy\n\ndef get_update_positions_input(state, input_variables):\n    prey_energy = get_var(state, input_variables['prey_energy'])\n    pred_energy = get_var(state, input_variables['pred_energy'])\n    prey_work = get_var(state, input_variables['prey_work'])\n    pred_work = get_var(state, input_variables['pred_work'])\n\n    return prey_energy, pred_energy, prey_work, pred_work\n\ndef get_find_eatable_grass_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    positions = get_var(state, input_variables['positions'])\n    grass_growth = get_var(state, input_variables['grass_growth'])\n\n    return bounds, positions, grass_growth\n\ndef get_eat_grass_input(state, input_variables):\n    bounds = get_var(state, input_variables['bounds'])\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    energy = get_var(state, input_variables['energy'])\n    nutrition = get_var(state, input_variables['nutrition'])\n    grass_growth = get_var(state, input_variables['grass_growth'])\n    growth_countdown = get_var(state, input_variables['growth_countdown'])\n    regrowth_time = get_var(state, input_variables['regrowth_time'])\n\n    return bounds, prey_pos, energy, nutrition, grass_growth, growth_countdown, regrowth_time\n\ndef get_find_targets_input(state, input_variables):\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    pred_pos = get_var(state, input_variables['pred_pos'])\n\n    return prey_pos, pred_pos\n\ndef get_hunt_prey_input(state, input_variables):\n    prey_pos = get_var(state, input_variables['prey_pos'])\n    prey_energy = get_var(state, input_variables['prey_energy'])\n    pred_pos = get_var(state, input_variables['pred_pos'])\n    pred_energy = get_var(state, input_variables['pred_energy'])\n    nutrition = get_var(state, input_variables['nutritional_value'])\n\n    return prey_pos, prey_energy, pred_pos, pred_energy, nutrition\n\ndef get_grow_grass_input(state, input_variables):\n    grass_growth = get_var(state, input_variables['grass_growth'])\n    growth_countdown = get_var(state, input_variables['growth_countdown'])\n\n    return grass_growth, growth_countdown\n</code></pre>"},{"location":"tutorials/creating-a-model/#move","title":"Move","text":"<p>First, we observe the state, and find a list of neighboring positions for each of the predators/prey currently alive.</p> <pre><code>@Registry.register_substep(\"find_neighbors\", \"observation\")\nclass FindNeighbors(SubstepObservation):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state):\n    bounds, adj_grid, positions = get_find_neighbors_input(state, self.input_variables)\n\n    # for each agent (prey/predator), find the adjacent cells and pass\n    # them on to the policy class.\n    possible_neighbors = []\n    for pos in positions:\n      possible_neighbors.append(\n        get_neighbors(pos, adj_grid, bounds)\n      )\n\n    return { self.output_variables[0]: possible_neighbors }\n</code></pre> <p>Then, we decide the course of action: to move each entity to a random neighboring position, only if they have the energy to do so.</p> <pre><code>@Registry.register_substep(\"decide_movement\", \"policy\")\nclass DecideMovement(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    positions, energy = get_decide_movement_input(state, self.input_variables)\n    possible_neighbors = observations['possible_neighbors']\n\n    # randomly choose the next position of the agent. if the agent\n    # has non-positive energy, don't let it move.\n    next_positions = []\n    for idx, pos in enumerate(positions):\n      next_positions.append(\n        random.choice(possible_neighbors[idx]) if energy[idx] &gt; 0 else pos\n      )\n\n    return { self.output_variables[0]: torch.stack(next_positions, dim=0) }\n</code></pre> <p>Lastly, we update the state, with the new positions of the entities, and reduce the energy of each entity by the value of the <code>stride_work</code> learnable parameter.</p> <pre><code>@Registry.register_substep(\"update_positions\", \"transition\")\nclass UpdatePositions(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    prey_energy, pred_energy, prey_work, pred_work = get_update_positions_input(state, self.input_variables)\n\n    # reduce the energy of the agent by the work required by them\n    # to take one step.\n    prey_energy = prey_energy + torch.full(prey_energy.shape, -1 * (prey_work.item()))\n    pred_energy = pred_energy + torch.full(pred_energy.shape, -1 * (pred_work.item()))\n\n    return {\n      self.output_variables[0]: action['prey']['next_positions'],\n      self.output_variables[1]: prey_energy,\n      self.output_variables[2]: action['predator']['next_positions'],\n      self.output_variables[3]: pred_energy\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#eat","title":"Eat","text":"<p>First, decide which grass is fit to be consumed by the prey.</p> <pre><code>@Registry.register_substep(\"find_eatable_grass\", \"policy\")\nclass FindEatableGrass(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    bounds, positions, grass_growth = get_find_eatable_grass_input(state, self.input_variables)\n\n    # if the grass is fully grown, i.e., its growth_stage is equal to\n    # 1, then it can be consumed by prey.\n    eatable_grass_positions = []\n    max_x, max_y = bounds\n    for pos in positions:\n      x, y = pos\n      node = (max_y * x) + y\n      if grass_growth[node] == 1:\n        eatable_grass_positions.append(pos)\n\n    # pass on the consumable grass positions to the transition class.\n    return { self.output_variables[0]: eatable_grass_positions }\n</code></pre> <p>Then, simulate the consumption of the grass, and update the growth stage, growth countdown, and energies of the grass and prey respectively.</p> <pre><code>@Registry.register_substep(\"eat_grass\", \"transition\")\nclass EatGrass(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    bounds, prey_pos, energy, nutrition, grass_growth, growth_countdown, regrowth_time = get_eat_grass_input(state, self.input_variables)\n\n    # if no grass can be eaten, skip modifying the state.\n    if len(action['prey']['eatable_grass_positions']) &lt; 1:\n      return {}\n\n    eatable_grass_positions = torch.stack(action['prey']['eatable_grass_positions'], dim=0)\n    max_x, max_y = bounds\n    energy_mask = None\n    grass_mask, countdown_mask = torch.zeros(*grass_growth.shape), torch.zeros(*growth_countdown.shape)\n\n    # for each consumable grass, figure out if any prey agent is at\n    # that position. if yes, then mark that position in the mask as\n    # true. also, for all the grass that will be consumed, reset the\n    # growth stage.\n    for pos in eatable_grass_positions:\n      x, y = pos\n      node = (max_y * x) + y\n\n      # TODO: make sure dead prey cannot eat\n      e_m = (pos == prey_pos).all(dim=1).view(-1, 1)\n      if energy_mask is None:\n        energy_mask = e_m\n      else:\n        energy_mask = e_m + energy_mask\n\n      grass_mask[node] = -1\n      countdown_mask[node] = regrowth_time - growth_countdown[node]\n\n    # energy + nutrition adds the `nutrition` tensor to all elements in\n    # the energy tensor. the (~energy_mask) ensures that the change is\n    # undone for those prey that did not consume grass.\n    energy = energy_mask*(energy + nutrition) + (~energy_mask)*energy\n\n    # these masks use simple addition to make changes to the original\n    # values of the tensors.\n    grass_growth = grass_growth + grass_mask\n    growth_countdown = growth_countdown + countdown_mask\n\n    return {\n      self.output_variables[0]: energy,\n      self.output_variables[1]: grass_growth,\n      self.output_variables[2]: growth_countdown\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#hunt","title":"Hunt","text":"<p>First, decide which prey are to be eaten.</p> <pre><code>@Registry.register_substep(\"find_targets\", \"policy\")\nclass FindTargets(SubstepAction):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, observations):\n    prey_pos, pred_pos = get_find_targets_input(state, self.input_variables)\n\n    # if there are any prey at the same position as a predator,\n    # add them to the list of targets to kill.\n    target_positions = []\n    for pos in pred_pos:\n      if (pos == prey_pos).all(-1).any(-1) == True:\n        target_positions.append(pos)\n\n    # pass that list of targets to the transition class.\n    return { self.output_variables[0]: target_positions }\n</code></pre> <p>Then, update the energies of both the prey and the predator.</p> <pre><code>@Registry.register_substep(\"hunt_prey\", \"transition\")\nclass HuntPrey(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    prey_pos, prey_energy, pred_pos, pred_energy, nutrition = get_hunt_prey_input(state, self.input_variables)\n\n    # if there are no targets, skip the state modifications.\n    if len(action['predator']['target_positions']) &lt; 1:\n      return {}\n\n    target_positions = torch.stack(action['predator']['target_positions'], dim=0)\n\n    # these are masks similars to the ones in `substeps/eat.py`.\n    prey_energy_mask = None\n    pred_energy_mask = None\n    for pos in target_positions:\n      pye_m = (pos == prey_pos).all(dim=1).view(-1, 1)\n      if prey_energy_mask is None:\n        prey_energy_mask = pye_m\n      else:\n        prey_energy_mask = prey_energy_mask + pye_m\n\n      pde_m = (pos == pred_pos).all(dim=1).view(-1, 1)\n      if pred_energy_mask is None:\n        pred_energy_mask = pde_m\n      else:\n        pred_energy_mask = pred_energy_mask + pde_m\n\n    # any prey that is marked for death should be given zero energy.\n    prey_energy = prey_energy_mask*0 + (~prey_energy_mask)*prey_energy\n    # any predator that has hunted should be given additional energy.\n    pred_energy = pred_energy_mask*(pred_energy + nutrition) + (~pred_energy_mask)*pred_energy\n\n    return {\n      self.output_variables[0]: prey_energy,\n      self.output_variables[1]: pred_energy\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#grow","title":"Grow","text":"<p>In this substep, we simply update the growth countdown of every grass object, and if the countdown has elapsed, we update the growth stage to <code>1</code>.</p> <pre><code>@Registry.register_substep(\"grow_grass\", \"transition\")\nclass GrowGrass(SubstepTransition):\n  def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n\n  def forward(self, state, action):\n    grass_growth, growth_countdown = get_grow_grass_input(state, self.input_variables)\n\n    # reduce all countdowns by 1 unit of time.\n    growth_countdown_mask = torch.full(growth_countdown.shape, -1)\n    growth_countdown = growth_countdown + growth_countdown_mask\n\n    # if the countdown has reached zero, set the growth stage to 1,\n    # otherwise, keep it zero.\n    grass_growth_mask = (growth_countdown &lt;= 0).all(dim=1)\n    grass_growth = grass_growth_mask*(1) + (~grass_growth_mask)*(0)\n\n    return {\n      self.output_variables[0]: grass_growth.view(-1, 1),\n      self.output_variables[1]: growth_countdown\n    }\n</code></pre>"},{"location":"tutorials/creating-a-model/#execution-configuration-registry-and-runner","title":"Execution: Configuration, Registry, and Runner","text":""},{"location":"tutorials/creating-a-model/#configuration","title":"Configuration","text":"<p>There are several parts to the configuration, written in a file traditionally called <code>config.yaml</code>. The following is a brief overview of all the major sections in the configuration file.</p> <pre><code># config.yaml\n# configuration for the predator-prey model.\n\nmetadata:\n  # device type, episode count, data files, etc.\n\nstate:\n  environment:\n    # variables/properties of the simulated enviroment.\n\n  agents:\n    # a list of agents in the simulation, and their properties.\n    # each property must be initialized by specifying a value\n    # or a generator function, and have a fixed tensor shape.\n\n  objects:\n    # a list of objects, similar to the agents list.\n\n  network:\n    # a list of interaction models for the simulation.\n    # could be a grid, or a directed graph, etc.\n\nsubsteps:\n  # a list of substeps\n  # each substep has a list of agents to run that substep for\n  # as well as the function, input and output variables for each\n  # part of that substep (observation, policy and transition)\n</code></pre> <p>The following is an example of defining a property in the configuration.</p> <pre><code>bounds:\n  name: 'Bounds'\n  learnable: false\n  shape: 2\n  dtype: 'int'\n  value:\n    - ${simulation_metadata.max_x} # you can refer to other parts of the config using\n    - ${simulation_metadata.max_y} # the template syntax, i.e., ${path.to.config.value}\n  initialization_function: null\n</code></pre> <p>Notice that to define one single property, we mentioned:</p> <ul> <li>the name of the property, here, <code>'bounds'</code>.</li> <li>whether or not the property is learnable, in this case, <code>false</code>.</li> <li>the shape of the tensor that stores the values, in this case, it is a   one-dimensional array of two elements: <code>(max_x, max_y)</code>.</li> <li>the value of the property, either by directly providing the value or by   providing a function that returns the value.</li> </ul> <p>The full configuration for the predator-prey model can be found here.</p> <pre><code># define helper functions used in the configuration\n\n@Registry.register_helper('map', 'network')\ndef map_network(params):\n  coordinates = (40.78264403323726, -73.96559413265355) # central park\n  distance = 550\n\n  graph = ox.graph_from_point(coordinates, dist=distance, simplify=True, network_type=\"walk\")\n  adjacency_matrix = nx.adjacency_matrix(graph).todense()\n\n  return graph, torch.tensor(adjacency_matrix)\n\n@Registry.register_helper('random_float', 'initialization')\ndef random_float(shape, params):\n  \"\"\"\n    Generates a `Tensor` of the given shape, with random floating point\n    numbers in between and including the lower and upper limit.\n  \"\"\"\n\n  max = params['upper_limit'] + 1 # include max itself.\n  min = params['lower_limit']\n\n  # torch.rand returns a tensor of the given shape, filled with\n  # floating point numbers in the range (0, 1]. multiplying the\n  # tensor by max - min and adding the min value ensure it's\n  # within the given range.\n  tens = (max - min) * torch.rand(shape) + min\n\n  return tens\n\n@Registry.register_helper('random_int', 'initialization')\ndef random_int(shape, params):\n  \"\"\"\n    Generates a `Tensor` of the given shape, with random integers in\n    between and including the lower and upper limit.\n  \"\"\"\n\n  max = math.floor(params['upper_limit'] + 1) # include max itself.\n  min = math.floor(params['lower_limit'])\n\n  # torch.randint returns the tensor we need.\n  tens = torch.randint(min, max, shape)\n\n  return tens\n</code></pre>"},{"location":"tutorials/creating-a-model/#registry-and-runner","title":"Registry and Runner","text":"<p>The code that executes the simulation uses the AgentTorch <code>Registry</code> and <code>Runner</code>, like so:</p> <pre><code>config = read_config('config-map.yaml')\nmetadata = config.get('simulation_metadata')\nnum_episodes = metadata.get('num_episodes')\nnum_steps_per_episode = metadata.get('num_steps_per_episode')\nnum_substeps_per_step = metadata.get('num_substeps_per_step')\n</code></pre> <p>The registry is stores all the classes and functions used by the model, and allows the runner to call them as needed when intializing the simulation and executing the substeps.</p> <pre><code>registry = Registry()\nregistry.register(read_from_file, 'read_from_file', 'initialization')\nregistry.register(grid_network, 'grid', key='network')\n</code></pre> <p>The runner intializes and executes the simulation for us. It also returns:</p> <ul> <li>a list of the learnable parameters, so we can run optimization functions on   them and use the optimized values for the next episode.</li> <li>the trajectory of the state so far, so we can visualize the state using   libraries like <code>matplotlib</code>.</li> </ul> <pre><code>runner = Runner(config, registry)\n</code></pre> <p> The source code for the visualizer used in the following block is given in the next section. </p> <pre><code>runner.init()\n\nfor episode in range(num_episodes):\n  runner.step(num_steps_per_episode)\n\n  final_states = list(filter(\n    lambda x: x['current_substep'] == str(num_substeps_per_step - 1),\n    runner.state_trajectory[-1]\n  ))\n  visualizer = Plot(metadata.get('max_x'), metadata.get('max_y'))\n  visualizer.plot(final_states)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"tutorials/creating-a-model/#visualization","title":"Visualization","text":"<p>You can plot the simulation in different ways. In this notebook, two such methods are demonstrated; the X-Y grid, and the OpenStreetMap plot.</p> <pre><code># display the gifs\n\nfrom IPython.display import HTML\n\nHTML(\"\"\"\n    &lt;table&gt;\n    &lt;tr&gt;&lt;td&gt;\n    &lt;video alt=\"grid\" autoplay&gt;\n        &lt;source src=\"../predator-prey.mp4\" type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n    &lt;/td&gt;&lt;td&gt;\n    &lt;img src=\"../predator-prey.gif\" alt=\"map\" /&gt;\n    &lt;/td&gt;&lt;/tr&gt;\n    &lt;/table&gt;\n\"\"\")\n</code></pre> <pre><code># render the map\n\nfrom IPython.display import display, clear_output\n\nimport time\nimport matplotlib\nimport matplotlib.pyplot as plotter\nimport matplotlib.patches as patcher\nimport contextily as ctx\n\n%matplotlib inline\n\nclass Plot:\n  def __init__(self, max_x, max_y):\n    # intialize the scatterplot\n    self.figure, self.axes = None, None\n    self.prey_scatter, self.pred_scatter = None, None\n    self.max_x, self.max_y = max_x, max_y\n\n    plotter.xlim(0, max_x - 1)\n    plotter.ylim(0, max_y - 1)\n    self.i = 0\n\n  def update(self, state):\n    graph = state['network']['agent_agent']['predator_prey']['graph']\n    self.coords = [(node[1]['x'], node[1]['y']) for node in graph.nodes(data=True)]\n    self.coords.sort(key=lambda x: -(x[0] + x[1]))\n\n    self.figure, self.axes = ox.plot_graph(graph, edge_linewidth=0.3, edge_color='gray', show=False, close=False)\n    ctx.add_basemap(self.axes, crs=graph.graph['crs'], source=ctx.providers.OpenStreetMap.Mapnik)\n    self.axes.set_axis_off()\n\n    # get coordinates of all the entities to show.\n    prey = state['agents']['prey']\n    pred = state['agents']['predator']\n    grass = state['objects']['grass']\n\n    # agar energy &gt; 0 hai... toh zinda ho tum!\n    alive_prey = prey['coordinates'][torch.where(prey['energy'] &gt; 0)[0]]\n    alive_pred = pred['coordinates'][torch.where(pred['energy'] &gt; 0)[0]]\n    # show only fully grown grass, which can be eaten.\n    grown_grass = grass['coordinates'][torch.where(grass['growth_stage'] == 1)[0]]\n\n    alive_prey_x, alive_prey_y = np.array([\n      self.coords[(self.max_y * pos[0]) + pos[1]] for pos in alive_prey\n    ]).T\n    alive_pred_x, alive_pred_y = np.array([\n      self.coords[(self.max_y * pos[0]) + pos[1]] for pos in alive_pred\n    ]).T\n\n    # show prey in dark blue and predators in maroon.\n    self.axes.scatter(alive_prey_x, alive_prey_y, c='#0d52bd', marker='.')\n    self.axes.scatter(alive_pred_x, alive_pred_y, c='#8b0000', marker='.')\n\n    # increment the step count.\n    self.i += 1\n    # show the current step count, and the population counts.\n    self.axes.set_title('Predator-Prey Simulation #' + str(self.i), loc='left')\n    self.axes.legend(handles=[\n      patcher.Patch(color='#fc46aa', label=str(self.i) + ' step'),\n      patcher.Patch(color='#0d52bd', label=str(len(alive_prey)) + ' prey'),\n      patcher.Patch(color='#8b0000', label=str(len(alive_pred)) + ' predators'),\n      # patcher.Patch(color='#d1ffbd', label=str(len(grown_grass)) + ' grass')\n    ])\n\n    display(plotter.gcf())\n    clear_output(wait=True)\n    time.sleep(1)\n\n  def plot(self, states):\n    # plot each state, one-by-one\n    for state in states:\n      self.update(state)\n\n    clear_output(wait=True)\n</code></pre>"},{"location":"tutorials/creating-archetypes/","title":"Creating Archetypes","text":"<p>This page shows how a user can build a useful archetype in minutes.</p>"},{"location":"tutorials/creating-archetypes/#what-is-an-archetype","title":"What is an archetype?","text":"<p>An archetype is a plug\u2011in decision module that turns agent context into a single numeric decision each step. It has a basic surface: <code>configure(...)</code>, <code>broadcast(...)</code>, <code>sample(...)</code>. It\u2019s backend\u2011agnostic: you can use rules, extend the skeleton class <code>MockLLM</code>, or bring in your own LLM to evaluate decisions.</p> <p>Setup:</p> <pre><code>import agent_torch.core.llm.template as lm\nfrom agent_torch.core.llm.archetype import Archetype\n</code></pre>"},{"location":"tutorials/creating-archetypes/#defining-your-archetype","title":"Defining your Archetype:","text":"<p>Archetype is created with a prompt, LLM object, and an n_arch parameter which is the number of archetypes to be created. This is used to calculate a distribution from which the outputs are then sampled.</p> <p>Example:</p> <pre><code>llm = MockLLM()\narchetype_n_2  = Archetype(prompt=\"Your age is {age}...\", llm=llm, n_arch=2)\narchetype_n_12 = Archetype(prompt=\"Your age is {age}...\", llm=llm, n_arch=12)\n</code></pre>"},{"location":"tutorials/creating-archetypes/#step-1-setting-up-your-llm","title":"Step 1: Setting up your LLM","text":"<p>First, let us set up the LLM. You can use any LLM of your choosing, provided you implement a call hook. This is the ony hook that is REQUIRED. </p> <pre><code>class MyLLM:\n    # optional:\n    def initialize_llm(self):\n        return self \n    def __call__(self, prompt_inputs):\n        # prompt_inputs: [{\"agent_query\": str, \"chat_history\": [...]}, ...]\n        # Return float-convertible outputs; you own parsing/validation\n        return [\"...\" for _ in prompt_inputs]\n</code></pre> <p>Optional:</p> <ul> <li>If your LLM exposes <code>initialize_llm(self)</code>, Archetype will call it on construction. Use it to configure your environment/API keys if needed.</li> <li>Use the call hook when your LLM wrapper is directly callable, or modify the prompt hook for SDKs that expose a named method instead.</li> </ul> <p>Additionally, you can extend the <code>MockLLM</code> class to overload these hooks and implement your LLM like so:</p> <pre><code>from agent_torch.core.llm.mock_llm import MockLLM\n\nclass MyLLM(MockLLM):\n    # Optional: run any one-time setup (keys, clients, etc.)\n    def initialize_llm(self):\n        self.client = SomeSDK(api_key=os.getenv(\"API_KEY\"))\n        return self\n\n    # Preferred entrypoint: make the wrapper callable\n    # prompt_list: list[{\"agent_query\": str, \"chat_history\": list}]\n    # return: list[str] or list[{\"text\": str}] of same length\n    def __call__(self, prompt_list):\n        outputs = []\n        for item in prompt_list:\n            query = item.get(\"agent_query\", \"\")\n            history = item[\"chat_history\"] \n            # Replace this with your SDK call and parsing\n            score = self._rng.uniform(self.low, self.high)\n            outputs.append({\"text\": f\"{score:.3f}\"})\n        return outputs\n\n    # If your SDK exposes a named method instead of __call__, implement prompt(...)\n    # Archetype will fall back to this if __call__ is missing.\n    # def prompt(self, prompt_list):\n    #     return self.__call__(prompt_list)\n</code></pre> <p>I/O contract:</p> <p>Input: A list of items, one per prompt in the form: {\"agent_query\": str, \"chat_history\": list}. Chat history can be [] if you do not wish to use the memory handler.</p> <p>Output: a list of the same length, one result per input, in the same order. Each result must be float-convertible (you manage formatting/validation). Preferred: return strings like \"0.12\" (or floats like 1.23). Also accepted: dicts with a \"text\" key, e.g., {\"text\": \"321.0\"}. Archetype parses to floats and returns a torch tensor.</p>"},{"location":"tutorials/creating-archetypes/#step-2-creating-a-prompt-for-archetype-through-a-template-object","title":"Step 2: Creating a prompt for Archetype through a Template object","text":"<p>The prompt field for archetype will accept either a simple string template or Template object. </p> <p>String template example:</p> <pre><code>arch = Archetype(prompt=\"Your age is {age}, gender is {gender}, and you are deciding whether to isolate or not.\", llm=MyLLM(), n_arch=1)\n</code></pre> <p>However, when you have external data and you want to optimize on prompts, consider extending the Template class. The Template class defines how to turn agent and external context data into a single prompt for the LLM.</p> <p>Here is an example dataframe:</p> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    \"Salary\": [65000, 43000, 120000],\n    \"Work_Week_Hours\": [60, 40, 80],\n    \"Satisfaction_Level\": [\"Not satisfied\", \"Partially satisfied\", \"Very satisfied\"]\n})\n</code></pre> <p>To create a template class extension that utilizes this dataframe, first create Variable objects to encapsulate them. Variable objects mark placeholders and allow you to make them learnable for further optimization down the line (see Optimizing Prompt Variables with P3O).</p> <p>The \"desc\" field is used for metadata regarding the slot. Set learnable to True when considering prompt optimizations, else set it to False. Additionally, if you are uncertain if data for a particular row/attribute exists, you may use the default field to specify what the default placeholder value should be (OPTIONAL).</p> <p>Ex.</p> <pre><code># lm is an alias for template\nsalary = lm.Variable(desc=\"salary\") # # by default, sets learnable to False, default param is set to an empty string\nwork_week = lm.Variable(desc=\"hours worked in a week\", learnable=True)\nsatisfaction = lm.Variable(desc=\"categorical satisfaction levels\", learnable=False, default=\"unknown satisfaction\")\n</code></pre> <p>//todo: population helpers are not implemented yet.</p> <p>Note: Variable objects are not limited to exclusively external data; Variable objects may reference population attributes as well, where Template will infer the data source. To view what population attributes exist, load a population like so, and then use the .view() utility:</p> <pre><code>from agent_torch.populations import astoria\nastoria.view() # prints all attributes relevant attributes with sample values\n</code></pre> <p>You can then register Variable objects to the attributes you determine are relevant.</p> <pre><code>age = lm.Variable(desc = age) # sets learnable = False, default to ''\n</code></pre> <p>Then when extending template, reference the Variable objects in the prompt hook.</p> <pre><code>class MyTemplate(lm.Template):\n\n    def __prompt__(self):\n        #set prompt_string field\n        self.prompt_string = \"I work for {work_week}, and get paid {salary}. I am {satisfaction} with my current job.\"\n\n</code></pre> <p>When considering higher level instructions such as a system prompt and output prompt, reference system_prompt and the output_prompt hook. These are purely optional and will default to empty strings if not used. </p> <pre><code>#inside MyTemplate\ndef __system_prompt__(self):\n    return \"Consider the following variables and determine your willingness to isolate.\"\ndef __output__(self):\n    return \"Answer on a scale from 0 - 1.0.\"\n</code></pre> <p>The Template object assembles them in order of: system_prompt -&gt; prompt -&gt; output_prompt for the LLM. Once this is set up, you can integrate your newly configured Template into Archetype.</p> <p>Here is an example of it put together:</p> <pre><code>import agent_torch.core.llm.template as lm\n\nclass MyTemplate(lm.Template):\n    salary = lm.Variable(desc=\"salary\")\n    work_week = lm.Variable(desc=\"hours worked in a week\", learnable=True)\n    satisfaction = lm.Variable(desc=\"categorical satisfaction levels\", learnable=False, default=\"unknown satisfaction\")\n\n    def __system_prompt__(self):\n        return \"Consider the following variables and determine your willingness to isolate.\"\n\n    def __prompt__(self):\n        self.prompt_string = \"I work for {work_week} hours, and get paid ${salary}. I am {satisfaction} with my current job.\"\n\n    def __output__(self):\n        return \"Answer on a scale from 0 - 1.0.\"\n\n\narch = Archetype(prompt=MyTemplate(), llm=MockLLM(), n_arch=3)\n\n# Example of a filled LLM prompt:\n# -------------------------------\n'''\n\"Consider the following variables and determine your willingness to isolate.\nI work for 60 hours and get paid $65000. I am Not Satisfied with my current job.\nAnswer on a scale from 0-1.0.\"\n'''\n</code></pre>"},{"location":"tutorials/creating-archetypes/#step-3-configuring-your-data","title":"Step 3: Configuring your data","text":"<p>This step is crucial in ensuring your data is fully compatible and wired to the Archetype object.  Utilize the .configure method to set your external data to the archetype. If you want extra flexibility for testing, use the split parameter to selectively choose how many rows to feed from.</p> <pre><code>arch.configure(external_df=df)\narch.configure(external_df=df, split=2)  # Only takes the first 2 rows of the df\n</code></pre>"},{"location":"tutorials/creating-archetypes/#step-4-using-archetype","title":"Step 4: Using Archetype","text":"<p>Archetype contains sampling and broadcasting functionalities. Use .sample when you want a quick preview of how your external data prompts might look, when you may not necessarily need per-agent outputs yet. Use .broadcast when you want to bind to a population for one value per agent in a given population. </p>"},{"location":"tutorials/creating-archetypes/#using-sample-pre-broadcast-vs-post-broadcast","title":"Using .sample() pre-broadcast vs post-broadcast","text":"<p>It is advisable to call .sample before calling .broadcast to determine if prompts are correctly configured for your external data source.</p> <p>When sampling from Archetype pre-broadcast call (if your data is configured correctly), it will render one prompt per dataframe row and produce a tensor in the shape of (n_rows,). If data is configured incorrectly or .configure() was not called, .sample() will return a shape of (1,). If split=k was called during configure, it will return a tensor in the shape of (k,).</p> <p>When using .sample(), use <code>verbose = True</code> to carefully inspect prompts. Note that when calling sample before binding to a population, population fields (such as age, gender, supplied by the population module) will remain as placeholders as archetype has not been bounded to a population yet. Only after broadcast is called, will it fill in the full prompt.</p> <pre><code>arch.sample()  # no prompt printing\n#OR\narch.configure(df, split = k)\narch.sample(verbose=True)  # prints k prompts\n</code></pre> <p>After you have used .broadcast(), you can call sample again to receive per-agent outputs in the shape of a (n_agents,) tensor.</p>"},{"location":"tutorials/creating-archetypes/#using-broadcast","title":"Using .broadcast()","text":"<p>If you would like to broadcast the prompts to a wider population, use the .broadcast() method. It expects a population object to broadcast to, a match_on argument to determine the proper key from the external dataframe for each agent, and a group-on argument which determines which batching key to use. By default, if the group-on is not specified, the group_on key will default to match_on. </p> <p>First, import your population.</p> <pre><code>from agent_torch.populations import astoria\n</code></pre> <p>Then determine what your external data should \"match_on\" to. If your external data contains n_rows for each n_agents in a population on a per-agent basis, disregard match_on. By default, if a match_on key is not provided, .broadcast() will index on a per-agent basis (one row in external data assigned to one agent). </p> <p>However, if your dataframe does not cover a per-agent basis, ensure that:</p> <p>The population chosen has a .pkl file which references the key you wish to map to. Template will fill the external data based on that key for each agent, so ensure your dataframe references the exact key. </p> <p>Assume we have created a Job_Number.pkl file in the \"astoria\" population which contain the numbers 1, 2, 3 and matches them across n_agents.</p> <p>In your dataframe, add a Job_Number column:</p> <pre><code>df = pd.DataFrame({\n    \"Job_Number\": [1,2,3], # Job 1 matches to an agent with the attributes: (salary, 65000), (Work_Week_Hours, 60), (Satisfaction_Level, \"Not Satisfied\")\n    \"Salary\": [65000, 43000, 120000],\n    \"Work_Week_Hours\": [60, 40, 80],\n    \"Satisfaction_Level\": [\"Not satisfied\", \"Partially satisfied\", \"Very satisfied\"]\n})\n'''\ncall broadcast on astoria, mapping external data to Job_Number attribute in agent data. Broadcast binds the population and joins external data by Job_Number. \n'''\narch.broadcast(astoria, match_on=\"Job_Number\")\n</code></pre> <p>Lastly, consider how to batch/group your prompts with an explicit group_on argument. The group_on argument creates buckets with the given keys, so that all agents in the same bucket share one prompt and receive the same score. Use a single key (e.g., \"Job_Number\") or a list (e.g., [\"Job_Number\", \"Age\"]). This reduces duplicate LLM calls, speeds up runs, and keeps decisions consistent for identical contexts.</p> <p>Examples of broadcast calls:</p> <pre><code>arch.broadcast(population=astoria, match_on=\"Job_Number\")  # groups based on match_on argument aka Job_Number\narch.broadcast(population=astoria, match_on=\"Job_Number\", group_on = \"Age\")         # any agent with the same age is put into a bucket\narch.broadcast(population=astoria, match_on=\"Job_Number\", group_on = [\"Age\", \"Job_Number\"])  # same age AND job_number share a bucket\n</code></pre> <p>Call .sample after calling .broadcast to execute on the buckets created. This step is necessary to execute if you want to receive an (n_agents,) decision tensor.</p>"},{"location":"tutorials/differentiable-discrete-sampling/","title":"Differentiable Discrete Sampling using AgentTorch","text":""},{"location":"tutorials/differentiable-discrete-sampling/#introduction","title":"Introduction","text":"<p>Discrete sampling poses significant challenges in gradient-based optimization due to its non-differentiable nature, which prevents effective backpropagation. Operations like argmax disrupt gradient flow, leading to high-variance or biased gradient estimates. The Gumbel-Softmax technique addresses this by using a reparameterization trick that adds Gumbel noise to logits and applies a temperature-controlled softmax, enabling differentiable approximations of discrete samples. As the temperature approaches zero, the method produces near-discrete outputs while maintaining gradient flow, making it suitable for integrating discrete sampling into neural networks.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#rethinking-discrete-sampling","title":"Rethinking Discrete Sampling","text":"<p>It was assumed that Gumbel softmax solves this problem. However, Gumbel-Softmax has its own limitations. The temperature parameter introduces a bias-variance tradeoff: higher temperatures smooth gradients but deviate from true categorical distributions, while lower temperatures yield near-discrete samples with unstable gradients. Additionally, its continuous approximations may require straight-through estimators, which can introduce bias during backpropagation. These issues make Gumbel-Softmax less effective in tasks requiring precise distribution matching or structured outputs, highlighting the need for further improvements in discrete sampling techniques.</p> <p>So we introduce a new method for discrete sampling using the <code>agent_torch.core.distribution.Categorical</code> class. This class provides a differentiable approximation to discrete sampling, allowing for gradient-based optimization while maintaining the integrity of the categorical distribution.</p> <p>This estimator can simply be used as follows:</p> <pre><code>import torch\nfrom agent_torch.core.distributions import Categorical\n# Define the probabilities for each category\nprobs = torch.tensor([0.2, 0.5, 0.3], dtype=torch.float32)\n# Create a Categorical distribution\nsample = Categorical.apply(probs)\n# The sample will be a tensor containing the sampled category\nprint(sample)\n</code></pre> <p>Let's discuss more about this by seeing its application in various experiments.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-1-random-walk","title":"Experiment 1: Random Walk","text":"<p>Let's implement a 1D markovian random walk X0, X1, ...., Xn using the <code>agent_torch.core.distribution.Categorical</code> sampling method. The agent can move left or right with probabilites:</p> <ul> <li>Xn+1 = Xn + 1 with probability e^(-Xn/p)</li> <li>Xn+1 = Xn - 1 with probability 1 - e^(-Xn/p)</li> </ul> <p>First, lets import the important modules:</p> <pre><code>import torch\nimport math\nfrom agent_torch.core.distributions import Categorical\n</code></pre> <p>We are interested in studying the asymptotic behavior of the variance of our automatically derived gradient estimator, and so set p = n so that the transition function varies appreciably over the range of the walk for all n.</p> <p>Let's define the main function:</p> <pre><code>def random_walk_categorical(n, p, device):\n    x = 0.0  # initial state\n    path = [0.0]\n    for _ in range(n):\n        # Compute the probability of moving up.\n        q = math.exp(-x / p)\n        prob = torch.tensor([q, 1.0 - q], dtype=torch.float32, device=device).unsqueeze(0)  \n        # Sample an action using the custom Categorical function.\n        sample = Categorical.apply(prob)  \n        move = 1 if sample.item() == 0 else -1\n        # if at x==0, a downward move is overridden, since probability for going up is 1.\n        if x == 0 and move == -1:\n            move = 1\n        x += move\n        path.append(x)\n    return path\n</code></pre> <p>This random walk can be generated by:</p> <pre><code>n = 20  # A 20 step simulation\np = n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nrandom_path = random_walk_categorical(n,p,device)\n\n# This random walk looks like [0,1,2,1,...]\n</code></pre> <p>Having seen how a random walk is implemented using AgentTorch, let's benchmark this against the Gumbel softmax method. The Gumbel softmax method is a differentiable approximation to the categorical distribution, allowing for gradient-based optimization. Let's discuss the experiment setup.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-setup","title":"Experiment Setup","text":"<p>This experiment focuses on the optimization of a parameter \u03b8 (theta) embedded within an exponential probability distribution function exp(-(x + \u03b8)/p), which governs the stochastic transition dynamics of our model. The primary objective is to calibrate \u03b8 such that the model's behavior closely approximates a baseline implementation, as measured by mean squared error (MSE).</p> <p>The methodology involves generating a substantial dataset comprising 1,000 trajectories, each consisting of 100 discrete time steps. This dataset is partitioned following standard machine learning protocols, with 70% allocated for parameter estimation (training) and 30% reserved for out-of-sample validation (testing).</p> <p>By systematically adjusting \u03b8, we aim to modulate the underlying probability distribution, thereby altering the likelihood of specific state transitions. This parameter optimization process seeks to minimize the discrepancy between the simulated trajectories and those produced by the baseline model. The efficacy of each candidate value for \u03b8 is quantitatively assessed via the MSE metric, which provides a rigorous measure of the deviation between the predicted and reference trajectories.</p> <p>This approach enables the fine-tuning of stochastic models to replicate observed phenomena with enhanced precision, with potential applications in various domains including statistical physics, financial modeling, and computational biology.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#results","title":"Results","text":"<p>The empirical findings demonstrate that the <code>agent_torch.core.distribution.Categorical</code> approach consistently exhibits superior performance metrics compared to the Gumbel-based method. Specifically, the <code>agent_torch.core.distribution.Categorical</code> method maintains consistently lower Wasserstein distance values across all experimental configurations, indicating better alignment between simulated and baseline distributions. Furthermore, the <code>agent_torch.core.distribution.Categorical</code> approach effectively preserves the variance ratio at approximately unity, which substantiates that the generated trajectories maintain distributional characteristics highly comparable to those of the baseline.</p> <p>Although the parameter convergence behavior varies across different initialization points, particularly for initial values of 10.0 and 0.0, the distributional properties of the Categorical method's outputs remain demonstrably superior to those produced by the Gumbel approach. This superiority is quantitatively verified through both lower Wasserstein distance measurements and reduced mean squared error metrics, which collectively indicate that the <code>agent_torch.core.distribution.Categorical</code> method generates distributions with greater fidelity to the baseline distribution regardless of initialization conditions. These results suggest that the <code>agent_torch.core.distribution.Categorical</code> approach provides a more robust framework for distribution matching in this experimental context, maintaining consistent performance advantages across varied experimental configurations.</p> <p>These results will further become clear when we plot these random walks. It can clearly be infered that the Gumbel method starts diverging from the baseline and performs poorly on the test dataset.</p> <p> </p> <p>First, we run the experiment for 100 time-steps and calibrate theta values for both methods. Among the methods, <code>agent_torch.core.distribution.Categorical</code> stays relatively close to the baseline, while the Gumbel-based approach begins to drift early and deviates substantially in the testing region. This suggests that the Categorical method generalizes better across regions and is more stable under extended evaluation.</p> <p>Second, we extend the experiment to 1000 steps to examine long-term behavior. Over this longer horizon, the difference becomes even more pronounced. Gumbel's trajectory continues to diverge and accumulates a large positional error, confirming its poor generalization performance. In contrast, <code>agent_torch.core.distribution.Categorical</code> remains much more aligned with the baseline throughout.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-2-neural-relational-inference","title":"Experiment 2: Neural Relational Inference","text":"<p>The neural relational inference experiment is designed to infer and model latent interactions among entities in a dynamic system. In this experiment, a graph-based neural architecture is employed in which a factor graph CNN encoder extracts relational features from observed data, while a learned recurrent interaction net decoder predicts future states by modeling interactions between nodes (or atoms). The goal is to simultaneously learn the underlying relations and use these learned interactions to improve prediction accuracy and interpretability of the system\u2019s dynamics.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#experiment-setup_1","title":"Experiment Setup","text":"<p>The NRI experiment specifically focuses on learning to infer interactions in physical systems without supervision. The model is structured as a variational auto-encoder where the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. The researchers conducted experiments on simulated physical systems including springs and charged particles. The model is evaluated on its ability to recover ground-truth interactions in these simulated environments, as well as its capacity to find interpretable structure and predict complex dynamics in real-world data such as motion capture and sports tracking data</p> <p>Initially, the experiment employed a Gumbel-Softmax approach for discrete sampling. In this setup, the addition of Gumbel noise and a temperature-controlled softmax allowed for differentiable approximations of categorical samples. However, the inherent bias-variance tradeoff\u2014where higher temperatures yield smoother but less discrete gradients, and lower temperatures produce near-discrete but unstable gradients\u2014limits the method's effectiveness. While the negative log-likelihood decreases over epochs, the KL divergence remains relatively low, suggesting insufficient regularization of the discrete structure .</p> <p>Recognizing these limitations, the experiment was repeated using our <code>agent_torch.core.distribution.Categorical</code> class. This new estimator directly provides a differentiable approximation for discrete sampling, bypassing some of the drawbacks inherent in the Gumbel-Softmax method. Notably, by more tightly coupling the sampling process to the categorical distribution, the estimator mitigates the bias-variance issue and improves gradient stability during training.</p>"},{"location":"tutorials/differentiable-discrete-sampling/#results_1","title":"Results","text":"<p>The training logs for the categorical estimator experiment reveal several improvements:</p> <ul> <li>Stable KL Divergence: The KL divergence values remained consistent from early epochs into convergence. This higher and stable KL value suggests that the model is enforcing a stronger regularization on the inferred discrete relations, leading to a more consistent latent structure.</li> <li>Lower Negative Log-Likelihood: While both methods converge to low nll_train values as training proceeds, the categorical estimator maintains comparably low loss values alongside improved training accuracy. </li> <li>Improved Predictive Accuracy: The accuracy trends in the logs show that the categorical estimator experiment reaches and sustains higher accuracy levels. The results point to a model that not only fits the data better but also generalizes more effectively\u2014an essential trait when dealing with structured, relational data.</li> </ul>"},{"location":"tutorials/differentiable-discrete-sampling/#conclusion","title":"Conclusion","text":"<p>This tutorial demonstrated how to implement and use differentiable discrete sampling operations using AgentTorch.</p>"},{"location":"tutorials/distributed-simulation/","title":"Distributed Multi-GPU Simulation","text":"<p>AgentTorch supports distributed simulation across multiple GPUs with minimal code changes.</p>"},{"location":"tutorials/distributed-simulation/#quick-start-only-2-lines-changed","title":"Quick Start: Only 2 Lines Changed!","text":"<p>Convert any existing AgentTorch simulation to distributed execution:</p> <pre><code># BEFORE (Single GPU)\nfrom agent_torch.populations import sample2\nfrom agent_torch.examples.models import movement\nfrom agent_torch.core.environment import envs\n\nrunner = envs.create(model=movement, population=sample2)\nrunner.init()\nrunner.step(20)\n</code></pre> <pre><code># AFTER (Distributed Multi-GPU) - Only 2 lines changed!\nfrom agent_torch.populations import sample2\nfrom agent_torch.examples.models import movement\nfrom agent_torch.core.environment import envs\n\nrunner = envs.create(\n    model=movement, \n    population=sample2, \n    distributed=True,           # &lt;-- CHANGE 1: Enable distributed\n    world_size=4               # &lt;-- CHANGE 2: Specify GPU count\n)\nrunner.init()  # Same as before\nrunner.step(20)  # Same as before\n</code></pre> <p>That's it! Your simulation now runs across 4 GPUs automatically.</p>"},{"location":"tutorials/distributed-simulation/#key-features","title":"Key Features","text":"<ul> <li>No config files needed - just change 2 parameters in <code>envs.create()</code></li> <li>Automatic agent partitioning across GPUs</li> <li>Zero model code changes required</li> <li>Backward compatible with all existing models</li> <li>Auto-fallback to single GPU if needed</li> <li>Same API - <code>.init()</code>, <code>.step()</code>, <code>.reset()</code> work identically</li> </ul> <p>Scale to hundreds of millions of agents seamlessly.</p>"},{"location":"tutorials/distributed-simulation/#usage-examples","title":"Usage Examples","text":""},{"location":"tutorials/distributed-simulation/#basic-distributed-simulation","title":"Basic Distributed Simulation","text":"<pre><code># Use all available GPUs\nrunner = envs.create(\n    model=your_model,\n    population=your_population,\n    distributed=True  # Auto-detects GPU count\n)\n</code></pre>"},{"location":"tutorials/distributed-simulation/#specific-gpu-count","title":"Specific GPU Count","text":"<pre><code># Use exactly 4 GPUs\nrunner = envs.create(\n    model=your_model,\n    population=your_population,\n    distributed=True,\n    world_size=4\n)\n</code></pre>"},{"location":"tutorials/distributed-simulation/#custom-sync-settings","title":"Custom Sync Settings","text":"<pre><code># Advanced: Custom synchronization\ndistributed_config = {\n    \"strategy\": \"data_parallel\",\n    \"sync_frequency\": 5  # Sync every 5 steps\n}\n\nrunner = envs.create(\n    model=your_model,\n    population=your_population,\n    distributed=True,\n    world_size=4,\n    distributed_config=distributed_config\n)\n</code></pre>"},{"location":"tutorials/distributed-simulation/#complete-working-example","title":"\ud83c\udfaf Complete Working Example","text":"<p>See <code>agent_torch/examples/run_movement_sim_distributed.py</code>:</p> <pre><code>def run_movement_simulation_distributed(world_size=2):\n    \"\"\"Run movement simulation on multiple GPUs.\"\"\"\n\n    # Only these lines changed from single GPU version:\n    runner = envs.create(\n        model=movement, \n        population=sample2, \n        distributed=True,           # Enable distributed\n        world_size=world_size       # Specify GPU count\n    )\n\n    # Everything else stays exactly the same:\n    runner.init()\n\n    sim_steps = runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"]\n    num_episodes = runner.config[\"simulation_metadata\"][\"num_episodes\"]\n\n    for episode in range(num_episodes):\n        if episode &gt; 0:\n            runner.reset()\n        runner.step(sim_steps)\n\n        # Print results\n        positions = runner.state[\"agents\"][\"citizens\"][\"position\"]\n        print(f\"Average position: {positions.mean(dim=0)}\")\n</code></pre>"},{"location":"tutorials/distributed-simulation/#how-it-works-behind-the-scenes","title":"\ud83d\udee0\ufe0f How It Works Behind The Scenes","text":"<ol> <li><code>envs.create()</code> detects <code>distributed=True</code> and creates a <code>DistributedRunnerWrapper</code></li> <li>Automatic agent partitioning - framework splits agents across GPUs</li> <li>Same API - wrapper provides identical interface to regular runner</li> <li>PyTorch multiprocessing - spawns processes across GPUs automatically</li> <li>Auto-synchronization - handles data gathering and state management</li> </ol>"},{"location":"tutorials/distributed-simulation/#requirements","title":"\ud83d\udcdd Requirements","text":"<ul> <li>Multiple CUDA-capable GPUs</li> <li>PyTorch with CUDA support  </li> <li>Existing AgentTorch model and population</li> </ul> <p>No additional configuration files or setup needed!</p>"},{"location":"tutorials/distributed-simulation/#advanced-configuration","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"tutorials/distributed-simulation/#partitioning-strategies","title":"Partitioning Strategies","text":"<pre><code># Data parallelism (default)\ndistributed_config = {\"strategy\": \"data_parallel\"}\n\n# Future: Spatial parallelism\ndistributed_config = {\"strategy\": \"spatial_parallel\"}\n</code></pre>"},{"location":"tutorials/distributed-simulation/#synchronization-control","title":"Synchronization Control","text":"<pre><code>distributed_config = {\n    \"sync_frequency\": 10,    # Sync every 10 steps\n    \"compression\": \"gzip\"    # Compress communications\n}\n</code></pre>"},{"location":"tutorials/distributed-simulation/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"tutorials/distributed-simulation/#check-gpu-availability","title":"Check GPU Availability","text":"<pre><code>import torch\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\n</code></pre>"},{"location":"tutorials/distributed-simulation/#memory-issues","title":"Memory Issues","text":"<ul> <li>Reduce agent count per GPU</li> <li>Increase sync frequency to reduce memory usage</li> <li>Use gradient checkpointing for large models</li> </ul>"},{"location":"tutorials/distributed-simulation/#performance-issues","title":"Performance Issues","text":"<ul> <li>Ensure agent count is divisible by GPU count</li> <li>Monitor load balance across GPUs</li> <li>Use appropriate sync frequency</li> </ul>"},{"location":"tutorials/distributed-simulation/#migration-guide","title":"\ud83d\udca1 Migration Guide","text":"<p>Converting existing AgentTorch code is trivial:</p> <ol> <li>Find your <code>envs.create()</code> call</li> <li>Add <code>distributed=True</code> </li> <li>Optionally add <code>world_size=N</code></li> <li>Done!</li> </ol> <p>No other changes needed - your existing model, population, and simulation logic work unchanged. </p>"},{"location":"tutorials/integrating-with-beckn/","title":"AgentTorch-Beckn Solar Model","text":""},{"location":"tutorials/integrating-with-beckn/#overview","title":"Overview","text":"<p>AgentTorch is a differentiable learning framework that enables you to run simulations with over millions of autonomous agents. Beckn is a protocol that enables the creation of open, peer-to-peer decentralized networks for pan-sector economic transactions.</p> <p>This model integrates Beckn with AgentTorch, to simulate a solar energy network in which households in a locality can decide to either buy solar panels and act as providers of solar energy, or decide to use the energy provided by other households instead of installing solar panels themselves.</p> <p></p> <p>A visualization of increase in net solar energy used per street.</p>"},{"location":"tutorials/integrating-with-beckn/#mapping-beckn-protocol-to-agenttorch","title":"Mapping Beckn Protocol to AgentTorch","text":""},{"location":"tutorials/integrating-with-beckn/#1-network","title":"1. Network","text":"<p>The participants in the Beckn network (providers, customers and gateways) are considered agents that interact with each other.</p>"},{"location":"tutorials/integrating-with-beckn/#2-operations","title":"2. Operations","text":"<p>The following operations are simulated as substeps:</p>"},{"location":"tutorials/integrating-with-beckn/#1-a-customer-will-search-and-select-a-provider","title":"1. a customer will <code>search</code> and <code>select</code> a provider","text":"<ul> <li>the customer selects the closest provider with the least price</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#2-the-customer-will-order-from-the-provider","title":"2. the customer will <code>order</code> from the provider","text":"<ul> <li>the customer orders basis their monthly energy demand</li> <li>the provider only confirms the order if it has the capacity to</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#3-the-provider-will-fulfill-the-order","title":"3. the provider will <code>fulfill</code> the order","text":"<ul> <li>the provider's capacity is reduced for the given step (~= 30 real days)</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#4-the-customer-will-pay-for-the-work-done","title":"4. the customer will <code>pay</code> for the work done","text":"<ul> <li>the provider's revenue is incremented, while the customer's wallet is deducted the same   amount.</li> <li>the amount to be paid is determined by the provider's price, multiplied by the amount of   energy supplied.</li> </ul>"},{"location":"tutorials/integrating-with-beckn/#5-the-provider-will-restock-their-solar-energy","title":"5. the provider will <code>restock</code> their solar energy","text":"<ul> <li>the amount of energy replenished SHOULD BE (TODO) dependent on the season as well as the   weather.</li> </ul> <p>Each of the substeps' code (apart from #5) is taken as-is from the AgentTorch Beckn integration.</p> <p>Note that while Beckn's API calls are asynchronous, the simulation assumes they are synchronous for simplicity.</p>"},{"location":"tutorials/integrating-with-beckn/#3-data","title":"3. Data","text":"<p>The data for this example model is currently sourced from various websites, mostly from data.boston.gov. However, the data should actually come from the Beckn Protocol's implementation of a solar network.</p>"},{"location":"tutorials/integrating-with-beckn/#running-the-model","title":"Running the Model","text":"<p>To run the model, clone the github repository first:</p> <pre><code># git clone --depth 1 --branch solar https://github.com/AgentTorch/agent-torch-beckn solar-netowkr\n</code></pre> <p>Then, setup a virtual environment and install all dependencies:</p> <pre><code># cd solar-network/\n# python -m venv .venv/bin/activate\n# . .venv/bin/activate\n# pip install -r requirements.txt\n</code></pre> <p>Once that is done, you can edit the configuration (<code>config.yaml</code>), and change the data used in the simulation by editing the simulation's data files (<code>data/simulator/{agent}/{property}.csv</code>).</p> <p>Then, open Jupyter Lab and open the <code>main.ipynb</code> notebook, and run all the cells.</p> <pre><code># pip install jupyterlab\n# jupyter lab\n</code></pre>"},{"location":"tutorials/integrating-with-beckn/#todos","title":"Todos","text":"<ul> <li>Add more visualizations (plots/graphs/heatmaps/etc.)</li> <li>Improve the data used for the simulation, reduce the number of random values.</li> <li>Add more detailed logic to the substeps, i.e., seasonal fluctuation in energy generation   and prices.</li> <li>Include and run a sample beckn instance to pull fake data from.</li> </ul>"},{"location":"tutorials/optimizing-on-prompts/","title":"Optimizing Prompt Variables with P3O","text":"<p>P3O is a lightweight policy\u2011gradient (REINFORCE\u2011style) optimizer that tunes discrete Template Variables (declared with <code>learnable=True</code>) to improve the decisions produced by an <code>Archetype</code>.</p>"},{"location":"tutorials/optimizing-on-prompts/#why-optimize-prompts","title":"Why optimize prompts?","text":"<p>If you wish to reduce prompt engineering by learning how to present context fields (direct, labeled, descriptive, etc.) or improve task-specific metrics without retraining your LLM, consider using the Variable learnable argument and P3O import.</p>"},{"location":"tutorials/optimizing-on-prompts/#how-p3o-works","title":"How P3O works","text":"<p>When defining a Template object, you get to choose which Variable objects the P3O optimizer gets to run on. By setting learnable to True, you allow P3O to receive these objects from the Template and optimize on them.</p> <pre><code>age = lm.Variable(desc=\"agent age\", learnable=True)\ngender = lm.Variable(desc=\"agent gender\", learnable = False)\n</code></pre> <p>Each learnable Variable holds trainable logits over a few presentation choices. </p> <pre><code>#as of right now, they are hard-coded in this format\n# inside Variable.py, example presentation choices:\n  return \"\"\n  return value\n  return f\"{field_name}: {value}\"\n  return f\"with {value}\"\n  return f\"The {field_name} is {value}\"\n#in the future, it would be possible to add markers to data so p3o can optimize on which specific markers for external data it will sample and choose from rather than being vague presentation choices i.e choosing a certain skill over another\n</code></pre> <p>// TODO: Optimize the presentation choices for prompt data.</p> <p>When you call <code>arch.sample(...)</code> to obtain predictions per group, you can construct a P3O object to use either:</p> <p>Use a Rewards provider: this is your custom reward function. P3O calls it each step with (group_keys, group_preds, arch) and expects a list[float] of rewards (one per group, same order). Use it when you want RL/bandit-style optimization or KPIs without labeled targets.</p> <p>Or use a targets provider: this includes target lookup. P3O calls it with (group_keys, arch) and expects a list[float] of targets (one per group). P3O then computes rewards via reward_fn(y, t) (default 1 \u2212 (y \u2212 t)^2). Use it for supervised-style optimization with labeled targets.</p> <p>The sections below will go in-depth in how to use both of these.</p>"},{"location":"tutorials/optimizing-on-prompts/#rewards-provider","title":"Rewards Provider","text":"<p>Use a rewards provider when you want to optimize for your own objective (KPIs, safety, cost, UX) instead of classic ground\u2011truth targets. It lets you compute a scalar reward per group from the model\u2019s predictions (and any external signals) without aligning or exposing targets. This is ideal for online feedback, composite metrics, or cases where \u201cground truth\u201d is undefined.</p> <p>This is how you may define a reward function and pass it to P3O.</p> <p>Example:</p> <pre><code>def rewards_provider(group_keys, group_preds, arch):\n    # Simple KPI: reward higher scores closer to 0.5\n    return [1.0 - (y - 0.5)**2 for y in group_preds]\n\nopt = P3O(archetype=arch, rewards_provider=rewards_provider)\n</code></pre>"},{"location":"tutorials/optimizing-on-prompts/#targets-provider-optional-alternative","title":"Targets Provider (optional alternative)","text":"<p>Define a targets provider when you already have labeled targets and want a reproducible, target\u2011based objective. It can be helpful when you need to switch or compare reward shapes easily via <code>reward_fn</code> without changing data plumbing. If you prefer clearer auditing of (key \u2192 target) mappings versus computing rewards inline, then opt for using a targets provider.</p> <p>Provide a targets provider to P3O that returns one float target per group key. P3O combines it with an optional <code>reward_fn(y, t)</code> (default <code>1 - (y - t)**2</code>). </p> <p>Signature:</p> <pre><code>def targets_provider(group_keys: list[str], arch) -&gt; list[float]:\n    ...\n</code></pre> <p>Example:</p> <pre><code>def targets_provider(group_keys, arch):\n    lookup = {\"13-2099.01\": 0.73}\n    return [lookup.get(k, 0.0) for k in group_keys]\n\nopt = P3O(archetype=arch, targets_provider=targets_provider, reward_fn=lambda y, t: 1.0 - (y - t)**2,)\n</code></pre> <p>Once more, define this only when you already have labeled targets and want a reproducible, target\u2011based objective.</p> <p>Here is an example with pre-defined ground-truth (align targets from a keyed CSV):</p> <pre><code>import pandas as pd\n\ngt_df = pd.read_csv(\"ground_truth.csv\")  # columns: soc_code, willingness\nlookup = {str(r[\"soc_code\"]): float(r[\"willingness\"]) for _, r in gt_df.iterrows()}\n\ndef targets_provider(group_keys, arch):\n    # Return one target per group in the same order\n    return [lookup.get(str(k), 0.0) for k in group_keys]\n\nopt = P3O(\n    archetype=arch,\n    targets_provider=targets_provider,\n    reward_fn=lambda y, t: 1.0 - (y - t)**2,\n)\n</code></pre>"},{"location":"tutorials/optimizing-on-prompts/#custom-reward-functions","title":"Custom reward functions","text":"<p>If you want to use a custom rewards function:</p> <p>Without targets (fully decoupled): define any reward on (group_keys, group_preds, arch)</p> <pre><code>def rewards_provider(group_keys, group_preds, arch):\n    # Example: maximize margin above a threshold and penalize variance\n    thr = 0.6\n    base = [max(0.0, y - thr) for y in group_preds]\n    # optional stabilization\n    mean_y = sum(group_preds) / max(1, len(group_preds))\n    var_penalty = 0.1 * sum((y - mean_y)**2 for y in group_preds)\n    return [b - var_penalty for b in base]\n\nopt = P3O(archetype=arch, rewards_provider=rewards_provider)\n</code></pre> <p>With targets (supervised-style): keep targets separate and swap the fitness easily</p> <pre><code>def targets_provider(group_keys, arch):\n    # Pull from a service/DB/cache; one target per group\n    return fetch_targets_for(group_keys)\n\n# Quadratic by default; switch to absolute or custom metric anytime\nopt = P3O(\n    archetype=arch,\n    targets_provider=targets_provider,\n    reward_fn=lambda y, t: 1.0 - abs(y - t),\n)\n</code></pre> <p>Notes: - If both providers are passed, <code>rewards_provider</code> is used. - Keep rewards bounded/normalized for stability (e.g., [0, 1] or [-1, 1]).</p>"},{"location":"tutorials/optimizing-on-prompts/#using-the-p3o-optimizer","title":"Using the P3O Optimizer","text":"<p>See also: the companion notebook <code>p3o_demo.ipynb</code> in this folder for an end-to-end example of decoupled rewards and targets providers with <code>Archetype</code>.</p> <p>Rewards-based variant:</p> <pre><code>opt = P3O(archetype=arch, rewards_provider=rewards_provider) # create the P3O object and supply your rewards provider\nfor i in range(n): #optimization loop runs n times\n  arch.sample(print_examples=0)   # populate last group outputs/keys\n  opt.step()                      # computes rewards, updates Variable logits, and renders optimized Template object  \n  opt.zero_grad() # clears gradients for next iteration\n</code></pre> <p>Targets-based variant:</p> <pre><code>opt = P3O(\n    archetype=arch,\n    targets_provider=targets_provider,\n    reward_fn=lambda y, t: 1.0 - (y - t)**2,\n)\nfor i in range(n):\n    arch.sample(print_examples=0)\n    opt.step()\n    opt.zero_grad()\n</code></pre> <p>Loop notes: - Always call <code>arch.sample()</code> before <code>opt.step()</code> so group keys/preds are fresh. - Use <code>print_examples=k</code> during debugging; keep it <code>0</code> for performance runs. - If you need to observe choices, set <code>verbose=True</code> in P3O to print selected indices and rewards.</p>"},{"location":"tutorials/processing-a-population/","title":"Tutorial: Generating Base Population and Household Data","text":"<p>This tutorial will guide you through the process of generating base population and household data for a specified region using census data. We\u2019ll use a <code>CensusDataLoader</code> class to handle the data processing and generation.</p>"},{"location":"tutorials/processing-a-population/#before-starting","title":"Before Starting","text":"<p>Make sure your <code>population data</code> and <code>household data</code> are in the prescribed format. Names of the column need to be same as shown in the excerpts.</p> <p>Lets see a snapshot of the data</p> <p><code>Population Data</code> is a dictionary containing two pandas DataFrames: '<code>age_gender</code>' and '<code>ethnicity</code>'. Each DataFrame provides demographic information for different areas and regions.</p> <p>The <code>age_gender</code> DataFrame provides a comprehensive breakdown of population data, categorized by area, gender, and age group.</p>"},{"location":"tutorials/processing-a-population/#columns-description","title":"Columns Description","text":"<ul> <li><code>area</code>: Serves as a unique identifier for each geographical area, represented   by a string (e.g., <code>'BK0101'</code>, <code>'SI9593'</code>).</li> <li><code>gender</code>: Indicates the gender of the population segment, with possible values   being <code>'female'</code> or <code>'male'</code>.</li> <li><code>age</code>: Specifies the age group of the population segment, using a string   format such as <code>'20t29'</code> for ages 20 to 29, and <code>'U19'</code> for those under 19   years of age.</li> <li><code>count</code>: Represents the total number of individuals within the specified   gender and age group for a given area.</li> <li><code>region</code>: A two-letter code that identifies the broader region encompassing   the area (e.g., <code>'BK'</code> for Brooklyn, <code>'SI'</code> for Staten Island).</li> </ul>"},{"location":"tutorials/processing-a-population/#example-entry","title":"Example Entry","text":"<p>Here is a sample of the data structure within the <code>age_gender</code> DataFrame:</p> area gender age count region BK0101 female 20t29 3396 BK BK0101 male 20t29 3327 BK <p>This example entry demonstrates the DataFrame's layout and the type of demographic data it contains, highlighting its utility for detailed population studies by age and gender.</p> <p>The <code>ethnicity</code> DataFrame is structured to provide detailed population data, segmented by both geographical areas and ethnic groups.</p>"},{"location":"tutorials/processing-a-population/#columns-description_1","title":"Columns Description","text":"<ul> <li><code>area</code>: A unique identifier assigned to each area, formatted as a string   (e.g., <code>'BK0101'</code>, <code>'SI9593'</code>). This identifier helps in pinpointing specific   locations within the dataset.</li> <li><code>ethnicity</code>: Represents the ethnic group of the population in the specified   area.</li> <li><code>count</code>: Indicates the number of individuals belonging to the specified ethnic   group within the area. This is an integer value representing the population   count.</li> <li><code>region</code>: A two-letter code that signifies the broader region that the area   belongs to (e.g., <code>'BK'</code> for Brooklyn, <code>'SI'</code> for Staten Island).</li> </ul>"},{"location":"tutorials/processing-a-population/#example-entry_1","title":"Example Entry","text":"<p>Below is an example of how the data is presented within the DataFrame:</p> area ethnicity count region BK0101 asian 1464 BK BK0101 black 937 BK <p>This example illustrates the structure and type of data contained within the <code>ethnicity</code> DataFrame, showcasing its potential for detailed demographic studies.</p> <p><code>Household Data</code> contains the following columns:</p> <ul> <li><code>area</code>: Represents a unique identifier for each area.</li> <li><code>people_num</code>: The total number of people within the area.</li> <li><code>children_num</code>: The number of children in the area.</li> <li><code>household_num</code>: The total number of households.</li> <li><code>family_households</code>: Indicates the number of households identified as family   households, highlighting family-based living arrangements.</li> <li><code>nonfamily_households</code>: Represents the number of households that do not fall   under the family households category, including single occupancy and unrelated   individuals living together.</li> <li><code>average_household_size</code>: The average number of individuals per household.</li> </ul> <p>Below is a sample excerpt:</p> area people_num children_num household_num family_households nonfamily_households average_household_size 100100 104 56 418 1 0 2.488038 100200 132 73 549 1 0 2.404372 100300 5 0 10 0 1 5.000000 <p>Now that we have verified our input, we can proceed to next steps!</p>"},{"location":"tutorials/processing-a-population/#step-1-set-up-file-paths","title":"Step 1: Set Up File Paths","text":"<p>First, we need to specify the paths to our data files.</p> <p>Make sure to replace the placeholder paths with the actual paths to your data files.</p> <pre><code># Path to the population data file. Update with the actual file path.\nPOPULATION_DATA_PATH = \"docs/tutorials/processing-a-population/sample_data/NYC/population.pkl\"\n\n# Path to the household data file. Update with the actual file path.\nHOUSEHOLD_DATA_PATH = \"docs/tutorials/processing-a-population/sample_data/NYC/household.pkl\"\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-2-define-age-group-mapping","title":"Step 2: Define Age Group Mapping","text":"<p>We\u2019ll define a mapping for age groups to categorize adults and children in the household data:</p> <pre><code>AGE_GROUP_MAPPING = {\n    \"adult_list\": [\"20t29\", \"30t39\", \"40t49\", \"50t64\", \"65A\"],  # Age ranges for adults\n    \"children_list\": [\"U19\"],  # Age range for children\n}\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-3-load-data","title":"Step 3: Load Data","text":"<p>Now, let\u2019s load the population and household data:</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Load household data\nHOUSEHOLD_DATA = pd.read_pickle(HOUSEHOLD_DATA_PATH)\n\n# Load population data\nBASE_POPULATION_DATA = pd.read_pickle(POPULATION_DATA_PATH)\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-4-set-up-additional-parameters","title":"Step 4: Set Up Additional Parameters","text":"<p>We\u2019ll set up some additional parameters that might be needed for data processing. These are not essential for generating population, but still good to know if you decide to use them in future.</p> <pre><code># Placeholder for area selection criteria, if any. Update or use as needed.\n# Example: area_selector = [\"area1\", \"area2\"]\n# This will be used to filter the population data to only include the selected areas.\narea_selector = None\n\n# Placeholder for geographic mapping data, if any. Update or use as needed.\ngeo_mapping = None\n</code></pre>"},{"location":"tutorials/processing-a-population/#step-5-initialize-the-census-data-loader","title":"Step 5: Initialize the Census Data Loader","text":"<p>Create an instance of the <code>CensusDataLoader</code> class:</p> <pre><code>from agent_torch.data.census.census_loader import CensusDataLoader\n\ncensus_data_loader = CensusDataLoader(n_cpu=8, use_parallel=True)\n</code></pre> <p>This initializes the loader with 8 CPUs and enables parallel processing for faster data generation.</p>"},{"location":"tutorials/processing-a-population/#step-6-generate-base-population-data","title":"Step 6: Generate Base Population Data","text":"<p>Generate the base population data for a specified region:</p> <pre><code>census_data_loader.generate_basepop(\n    input_data=BASE_POPULATION_DATA,  # The population data frame\n    region=\"astoria\",  # The target region for generating base population\n    area_selector=area_selector,  # Area selection criteria, if applicable\n)\n</code></pre> <p>This will create a base population of 100 individuals for the \u201castoria\u201d region. The generated data will be exported to a folder named \u201castoria\u201d under the \u201cpopulations\u201d folder.</p>"},{"location":"tutorials/processing-a-population/#overview-of-the-generated-base-population-data","title":"Overview of the Generated Base Population Data","text":"<p>Each row corresponds to attributes of individual residing in the specified region while generating the population.</p> area age gender ethnicity region BK0101 20t29 female black BK BK0101 20t29 female hispanic BK ... ... ... ... ... BK0101 U19 male asian SI BK0101 U19 female white SI BK0101 U19 male asian SI"},{"location":"tutorials/processing-a-population/#step-7-generate-household-data","title":"Step 7: Generate Household Data","text":"<p>Finally, generate the household data for the specified region:</p> <pre><code>census_data_loader.generate_household(\n    household_data=HOUSEHOLD_DATA,  # The loaded household data\n    household_mapping=AGE_GROUP_MAPPING,  # Mapping of age groups for household composition\n    region=\"astoria\"  # The target region for generating households\n)\n</code></pre> <p>This will create household data for the \u201castoria\u201d region based on the previously generated base population. The generated data will be exported to the same \u201castoria\u201d folder under the \u201cpopulations\u201d folder.</p>"},{"location":"tutorials/processing-a-population/#bonus-generate-population-data-of-specific-size","title":"Bonus: Generate Population Data of Specific Size","text":"<p>For quick experimentation, this may come in handy.</p> <pre><code>census_data_loader.generate_basepop(\n    input_data=BASE_POPULATION_DATA,  # The population data frame\n    region=\"astoria\",  # The target region for generating base population\n    area_selector=area_selector,  # Area selection criteria, if applicable\n    num_individuals = 100 # Saves data for first 100 individuals, from the generated population\n)\n</code></pre>"},{"location":"tutorials/processing-a-population/#bonus-export-population-data","title":"Bonus: Export Population Data","text":"<p>If you have already generated your synthetic population, you just need to export it to \"populations\" folder under the desired \"region\", in order for you to use it with AgentTorch.</p> <pre><code>POPULATION_DATA_PATH = \"/population_data.pickle\"  # Replace with actual path\ncensus_data_loader.export(population_data_path=POPULATION_DATA_PATH,region=\"astoria\")\n</code></pre> <p>In case you want to export data for only few individuals</p> <pre><code>census_data_loader.export(population_data_path=POPULATION_DATA_PATH,region=\"astoria\",num_individuals = 100)\n</code></pre>"},{"location":"tutorials/processing-a-population/#conclusion","title":"Conclusion","text":"<p>You have now successfully generated both base population and household data for the <code>\u201castoria\u201d</code> region. The generated data can be found in the <code>\u201cpopulations/astoria\u201d</code> folder. You can modify the region name, population size, and other parameters to generate data for different scenarios.</p>"},{"location":"tutorials/runner_optimizations/","title":"Index","text":""},{"location":"tutorials/runner_optimizations/#runner-flow-base-vs-optimized","title":"Runner Flow: Base vs Optimized","text":"<p>This document compares the base Runner usage pattern with the optimized Runner and highlights the practical differences, why they matter, and how to use the optimized path.</p>"},{"location":"tutorials/runner_optimizations/#summary-of-differences","title":"Summary of Differences","text":"<ul> <li>Device awareness:</li> <li>Basic: CPU-oriented, no device attribute.</li> <li> <p>Optimized: Detects CUDA (<code>use_gpu</code>), sets <code>self.device</code>, and routes to GPU/CPU paths.</p> </li> <li> <p>Initialization:</p> </li> <li>Both: use <code>Initializer.initialize()</code> and record an initial CPU snapshot via <code>to_cpu(self.state)</code>.</li> <li> <p>Optimized: wires a pooled buffer allocator into transition modules on CUDA (<code>_wire_transition_buffer_allocator</code>).</p> </li> <li> <p>Step execution:</p> </li> <li>Basic: single <code>step(...)</code> loop with per-substep observe \u2192 act \u2192 progress, always snapshot to CPU.</li> <li> <p>Optimized: dispatches to <code>_step_cpu_base</code> (same as basic) or <code>_step_gpu_optimized</code> with vectorization, memory pooling, and reduced transfers.</p> </li> <li> <p>Performance instrumentation:</p> </li> <li>Basic: none.</li> <li> <p>Optimized: <code>perf_stats</code> (gpu_to_cpu_transfers, tensor_allocations, memory_reused, vectorized_operations) and <code>get_performance_stats()</code> helper.</p> </li> <li> <p>Parameter updates:</p> </li> <li>Basic: <code>_set_parameters</code> replaces tensors directly.</li> <li>Optimized: <code>_set_parameters</code> ensures replacement tensors are moved to the correct device.</li> </ul>"},{"location":"tutorials/runner_optimizations/#basic-pattern-minimal-cpu","title":"Basic pattern (minimal CPU)","text":"<pre><code>from agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\n\nfrom agent_torch.models import covid\nfrom agent_torch.populations import astoria\n\npop_loader = LoadPopulation(astoria)\nsimulation = Executor(model=covid, pop_loader=pop_loader)\nrunner = simulation.runner\n\nrunner.init()\nrunner.step(runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"])\n</code></pre>"},{"location":"tutorials/runner_optimizations/#optimized-pattern-auto-selects-best-runner-timings-stats","title":"Optimized pattern (auto-selects best runner, timings, stats)","text":"<pre><code>from agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation, DataLoader\nfrom agent_torch.models import covid\nfrom agent_torch.populations import astoria\nimport time\n\n# Setup\nt0 = time.perf_counter()\npop_loader = LoadPopulation(astoria)\ndl = DataLoader(covid, pop_loader)  # lets Executor pick an optimized Runner when available\nsimulation = Executor(model=covid, data_loader=dl)\nrunner = simulation.runner\n\nrunner.init()\n\n# Simulate and optionally print perf stats\nnum_steps = runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"]\nrunner.step(num_steps)\nif hasattr(runner, 'get_performance_stats'):\n    stats = runner.get_performance_stats()\n    print(\"\\nPerformance Stats:\")\n    for k, v in stats.items():\n        print(f\"   {k}: {v}\")\n</code></pre>"},{"location":"tutorials/runner_optimizations/#devicesafe-parameter-updates","title":"Device\u2011safe parameter updates","text":"<pre><code># Ensure new tensors match runner.device when setting parameters directly\ndevice = runner.device\nnew_tensor = torch.tensor([3.5, 4.2, 5.6], requires_grad=True, device=device)\n\nparam_path = \"initializer.transition_function.0.new_transmission.learnable_args.R2\"\nrunner._set_parameters({param_path: new_tensor})\n</code></pre>"},{"location":"tutorials/runner_optimizations/#added-optimizations-optimized-runner","title":"Added Optimizations (optimized Runner)","text":"<ol> <li>Device awareness and snapshot stream</li> <li>Fields: <code>_snapshot_stream</code>, <code>_batch_size</code>, <code>_pool_limit_per_shape</code>, <code>_inplace_progress</code></li> <li> <p>Purpose: manage memory and transfers efficiently and asynchronously when supported</p> </li> <li> <p>Memory pooling and reuse</p> </li> <li><code>_get_pooled_tensor</code>, <code>_return_to_pool</code></li> <li> <p>Reduces allocations; tracks <code>memory_reused</code> and <code>tensor_allocations</code></p> </li> <li> <p>Vectorized substep processing</p> </li> <li><code>_process_substep_vectorized</code>, <code>_gpu_optimized_agent_processing</code></li> <li> <p>Batches observation/action updates over active indices; increments <code>vectorized_operations</code></p> </li> <li> <p>Active-set detection</p> </li> <li> <p><code>_compute_active_indices</code> identifies which agents to process (e.g., by disease stage)</p> </li> <li> <p>Snapshot compression</p> </li> <li> <p><code>_compress_state_for_snapshot</code> downcasts/normalizes tensors; uses non-blocking transfers on a dedicated stream</p> </li> <li> <p>Optimized progress</p> </li> <li><code>_progress_state_optimized</code> placeholder for in-place/vectorized transitions (currently calls controller.progress)</li> </ol>"},{"location":"tutorials/runner_optimizations/#function-by-function-mapping-current-runner","title":"Function-by-Function Mapping (current Runner)","text":"<ul> <li><code>init()</code> \u2192 initialize and record initial CPU snapshot; wire buffer allocator on CUDA</li> <li><code>step(num_steps=None)</code> \u2192 dispatch to CPU base or GPU optimized path</li> <li><code>_step_cpu_base(...)</code> \u2192 mirrors the basic loop</li> <li><code>_step_gpu_optimized(...)</code> \u2192 vectorization, pooling, snapshot compression, perf counters</li> <li><code>_process_substep_vectorized(...)</code>, <code>_gpu_optimized_agent_processing(...)</code> \u2192 batched observe/act</li> <li><code>_observe_with_batches(...)</code>, <code>_act_with_batches(...)</code>, <code>_process_tensor_active_batched(...)</code> \u2192 active-index updates and reuse</li> <li><code>_wire_transition_buffer_allocator()</code> \u2192 pooled buffer hooks for transitions</li> <li><code>_get_pooled_tensor(...)</code>, <code>_return_to_pool(...)</code> \u2192 memory pool</li> <li><code>get_performance_stats()</code> \u2192 perf summary (optimized) or mode (base)</li> </ul>"},{"location":"tutorials/runner_optimizations/#when-to-use-the-optimized-pattern","title":"When to use the optimized pattern","text":"<ul> <li>Large populations (tens of thousands of agents)</li> <li>CUDA available (GPU compute)</li> <li>Need basic performance stats with minimal code changes</li> </ul>"},{"location":"tutorials/runner_optimizations/#using-the-optimized-runner","title":"Using the Optimized Runner","text":"<p>Below is a compact, copy\u2011pasteable pattern to set up and profile the optimized Runner without referring to any specific examples.</p> <pre><code>from agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation, DataLoader\nfrom agent_torch.models import covid\nfrom agent_torch.populations import astoria\nimport time\n\ndef setup(model, population):\n    # Timers\n    t0 = time.perf_counter()\n\n    # Executor + DataLoader \u2192 auto-select optimized Runner when available\n    t_loader_start = time.perf_counter()\n    pop_loader = LoadPopulation(population)\n    dl = DataLoader(model, pop_loader)\n    simulation = Executor(model=model, data_loader=dl)\n    t_loader_end = time.perf_counter()\n\n    runner = simulation.runner\n\n    # Time runner.init()\n    t_init_start = time.perf_counter()\n    runner.init()\n    t_init_end = time.perf_counter()\n\n    # Print init timing summary\n    loader_exec_s = t_loader_end - t_loader_start\n    runner_init_s = t_init_end - t_init_start\n    total_init_s = t_init_end - t0\n    print(f\"\\n Init timings: loader+executor={loader_exec_s:.3f}s, runner.init()={runner_init_s:.3f}s, total={total_init_s:.3f}s\")\n\n    return runner\n</code></pre>"},{"location":"tutorials/runner_optimizations/#running-a-simulation-step-and-collecting-performance-stats","title":"Running a simulation step and collecting performance stats","text":"<pre><code>def simulate(runner):\n    num_steps_per_episode = runner.config[\"simulation_metadata\"][\"num_steps_per_episode\"]\n\n    runner.step(num_steps_per_episode)\n    traj = runner.state_trajectory[-1][-1]\n    preds = traj[\"environment\"][\"daily_infected\"]\n    loss = preds.sum()\n\n    # Print performance stats if available\n    if hasattr(runner, 'get_performance_stats'):\n        stats = runner.get_performance_stats()\n        print(f\"\\nPerformance Stats:\")\n        for key, value in stats.items():\n            print(f\"   {key}: {value}\")\n\n    return loss\n</code></pre>"},{"location":"tutorials/runner_optimizations/#devicesafe-parameter-updates_1","title":"Device\u2011safe parameter updates","text":"<p>When setting learnable parameters directly on substeps, ensure new tensors are on the same device as the Runner.</p> <pre><code>runner = setup(covid, astoria)\n\n# Get a device\u2011correct tensor\ndevice = runner.device\nnew_tensor = torch.tensor([3.5, 4.2, 5.6], requires_grad=True, device=device)\n\n# Example: map a dotted path to a learnable parameter and set it\ninput_string = \"initializer.transition_function.0.new_transmission.learnable_args.R2\"\nparams_dict = {input_string: new_tensor}\nrunner._set_parameters(params_dict)\n</code></pre>"},{"location":"tutorials/runner_optimizations/#endtoend-script-timing","title":"End\u2011to\u2011end script timing","text":"<p>You can also measure total script time and per\u2011section timings to profile initialization and simulation step performance.</p> <pre><code>if __name__ == \"__main__\":\n    script_t0 = time.perf_counter()\n    runner = setup(covid, astoria)\n\n    sim_t0 = time.perf_counter()\n    loss = simulate(runner)\n    sim_t1 = time.perf_counter()\n    print(f\"\\nLoss: {loss}\")\n\n    script_t1 = time.perf_counter()\n    print(f\"\\n Timings: simulation_step={sim_t1 - sim_t0:.3f}s, script_total={script_t1 - script_t0:.3f}s\")\n</code></pre>"},{"location":"tutorials/runner_optimizations/#key-differences-vs-base-pattern","title":"Key differences vs base pattern","text":"<ul> <li>Uses <code>DataLoader</code> so the <code>Executor</code> can choose the best Runner automatically.</li> <li>Tracks timings around loader/executor creation and <code>runner.init()</code>.</li> <li>Optionally prints performance stats via <code>runner.get_performance_stats()</code>.</li> <li>Ensures tensors are created on <code>runner.device</code> when directly setting parameters.</li> </ul> <p>Use this pattern when you want the fastest available execution and basic profiling hooks with minimal changes to your workflow.</p>"},{"location":"tutorials/runner_optimizations/#initialization-differences-vs-base-initializer","title":"Initialization differences vs base Initializer","text":"<p>The optimized Initializer accelerates and hardens setup compared to the legacy/base version:</p> <ul> <li>Device resolution and propagation</li> <li>Base: reads <code>device</code> directly from config</li> <li> <p>Optimized: supports <code>device: auto</code>, resolves to <code>cuda</code> when available and writes the resolved device back to <code>config[\"simulation_metadata\"][\"device\"]</code> so downstream code is consistent</p> </li> <li> <p>Streamed host\u2192device transfers (CUDA)</p> </li> <li>Base: <code>tensor.to(self.device)</code> for every tensor</li> <li> <p>Optimized: round\u2011robin CUDA streams + pinned memory + <code>non_blocking=True</code> to overlap copies     <code>python     def _to_device_streamed(self, cpu_tensor):         if not self.is_cuda: return cpu_tensor.to(self.device)         s = self._next_stream()         if hasattr(cpu_tensor, 'is_pinned') and not cpu_tensor.is_pinned():             cpu_tensor = cpu_tensor.pin_memory()         with torch.cuda.stream(s):             return cpu_tensor.to(self.device, non_blocking=True)</code></p> </li> <li> <p>Centralized device routing</p> </li> <li>Base: scattered <code>.to(self.device)</code> calls</li> <li> <p>Optimized: <code>_to_device()</code> routes through the streamed path when on CUDA, used consistently across environment/agents/objects/networks</p> </li> <li> <p>Network adjacency handling</p> </li> <li>Base: moves <code>edge_list</code>/<code>attr_list</code> directly</li> <li> <p>Optimized: moves via streamed path and guards tuple structure</p> </li> <li> <p>Clear error behavior for dynamic args</p> </li> <li>Base: prints a message and returns</li> <li> <p>Optimized: raises <code>NotImplementedError</code> so misconfigurations fail fast during init</p> </li> <li> <p>ParameterDict state</p> </li> <li>Base: stores a custom dict; pickling paths vary</li> <li>Optimized: tracks <code>nn.ParameterDict</code> for learnables; <code>__getstate__/__setstate__</code> save/restore a real <code>state_dict</code></li> </ul> <p>These changes collectively reduce init time and make initialization deterministic and observable, especially on GPUs.</p>"},{"location":"tutorials/simulation-and-calibration/","title":"Creating and Calibrating a Simulation","text":"<p>This tutorial will guide you through the process of creating a simulation instance and integrating it with calibration logic using AgentTorch. We'll cover:</p> <ol> <li>Setting up a basic simulation</li> <li>Configuring simulation parameters</li> <li>Running the simulation</li> <li>Integrating with calibration</li> <li>Advanced parameter tuning</li> </ol>"},{"location":"tutorials/simulation-and-calibration/#prerequisites","title":"Prerequisites","text":"<p>Before starting this tutorial, make sure you have:</p> <ul> <li>AgentTorch installed (<code>pip install agent-torch</code>)</li> <li>Basic understanding of Python and PyTorch</li> <li>Your model and population data ready</li> </ul>"},{"location":"tutorials/simulation-and-calibration/#basic-simulation-setup","title":"Basic Simulation Setup","text":"<p>First, let's create a basic simulation instance. Here's a minimal example:</p> <pre><code>from agent_torch.core.executor import Executor\nfrom agent_torch.core.dataloader import LoadPopulation\nfrom agent_torch.models import covid  # Example model\nfrom agent_torch.populations import sample2  # Example population\n\ndef setup_simulation(model, population):\n    # Create a population loader\n    loader = LoadPopulation(population)\n\n    # Initialize the simulation executor\n    simulation = Executor(model=model, pop_loader=loader)\n\n    # Get the runner instance\n    runner = simulation.runner\n\n    # Initialize the simulation\n    runner.init()\n\n    return runner\n\n# Create the simulation instance\nrunner = setup_simulation(covid, sample2)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#configuring-parameters","title":"Configuring Parameters","text":"<p>You can configure simulation parameters in two ways:</p> <ol> <li>During initialization:</li> </ol> <pre><code>simulation_config = {\n    'simulation_metadata': {\n        'num_steps_per_episode': 100,\n        'num_episodes': 1\n    }\n}\nrunner = setup_simulation(covid, sample2, config=simulation_config)\n</code></pre> <ol> <li>After initialization using the parameter API:</li> </ol> <pre><code>def set_parameter(runner, param_path, new_value):\n    \"\"\"\n    param_path: String path to the parameter (e.g., 'initializer.transition_function.0.new_transmission.learnable_args.R0')\n    new_value: New tensor value for the parameter\n    \"\"\"\n    params_dict = {param_path: new_value}\n    runner._set_parameters(params_dict)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#running-the-simulation","title":"Running the Simulation","text":"<p>To run the simulation:</p> <pre><code>def run_simulation(runner):\n    # Get simulation parameters\n    num_steps = runner.config['simulation_metadata']['num_steps_per_episode']\n\n    # Run simulation steps\n    runner.step(num_steps)\n\n    # Get final trajectory\n    final_trajectory = runner.state_trajectory[-1][-1]\n\n    return final_trajectory\n\n# Run simulation and get results\nresults = run_simulation(runner)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#integrating-with-calibration","title":"Integrating with Calibration","text":"<p>For calibration, we need to: 1. Define parameters to calibrate 2. Create a loss function 3. Set up optimization</p> <p>Here's how to do it:</p> <pre><code>import torch\nimport torch.optim as optim\n\ndef setup_calibration(runner):\n    # Get learnable parameters\n    learn_params = [(name, params) for (name, params) in runner.named_parameters()]\n\n    # Create optimizer\n    optimizer = optim.Adam(runner.parameters(), lr=0.01)\n\n    return optimizer\n\ndef calibration_step(runner, optimizer, target_data):\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Run simulation\n    trajectory = run_simulation(runner)\n\n    # Calculate loss (example using infected counts)\n    preds = trajectory['environment']['daily_infected']\n    loss = torch.nn.functional.mse_loss(preds, target_data)\n\n    # Backward pass\n    loss.backward()\n\n    # Update parameters\n    optimizer.step()\n\n    return loss.item()\n\n# Setup calibration\noptimizer = setup_calibration(runner)\n\n# Run calibration loop\nfor epoch in range(100):\n    loss = calibration_step(runner, optimizer, target_data)\n    print(f\"Epoch {epoch}, Loss: {loss}\")\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#advanced-parameter-tuning","title":"Advanced Parameter Tuning","text":"<p>For more fine-grained control over parameters:</p> <pre><code>def get_parameter(runner, param_path):\n    \"\"\"Get current parameter value\"\"\"\n    tensor_func = map_and_replace_tensor(param_path)\n    return tensor_func(runner)\n\ndef update_parameter(runner, param_path, new_value):\n    \"\"\"Update specific parameter with gradient tracking\"\"\"\n    assert isinstance(new_value, torch.Tensor)\n    assert new_value.requires_grad\n    set_parameter(runner, param_path, new_value)\n</code></pre>"},{"location":"tutorials/simulation-and-calibration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Always validate parameter changes:    <code>python    def validate_parameter(runner, param_path, new_value):        current_value = get_parameter(runner, param_path)        assert new_value.shape == current_value.shape, \"Shape mismatch\"        assert new_value.requires_grad == current_value.requires_grad, \"Gradient requirement mismatch\"</code></p> </li> <li> <p>Save and load calibrated parameters:    ```python    def save_parameters(runner, filepath):        torch.save(runner.state_dict(), filepath)</p> </li> </ol> <p>def load_parameters(runner, filepath):        runner.load_state_dict(torch.load(filepath))    ```</p>"},{"location":"tutorials/simulation-and-calibration/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>Gradient Issues: If you encounter gradient-related errors, ensure all parameters that need gradients have <code>requires_grad=True</code>.</p> </li> <li> <p>Memory Management: For large simulations, consider using:    <code>python    def clear_memory(runner):        runner.state_trajectory = []  # Clear trajectory history        torch.cuda.empty_cache()  # If using GPU</code></p> </li> <li> <p>Parameter Bounds: Implement parameter constraints:    <code>python    def constrain_parameters(runner, param_path, min_val, max_val):        value = get_parameter(runner, param_path)        constrained_value = torch.clamp(value, min_val, max_val)        set_parameter(runner, param_path, constrained_value)</code></p> </li> </ol>"},{"location":"tutorials/simulation-and-calibration/#next-steps","title":"Next Steps","text":"<ul> <li>Explore more advanced calibration techniques in the Calibration Advanced Guide</li> <li>Learn about analyzing simulation results in the Simulation Analysis Tutorial</li> <li>Understand how to integrate custom models in the Custom Models Guide </li> </ul>"}]}